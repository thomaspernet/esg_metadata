{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# US Name\n",
    "Model estimate Estimate sign of effect\n",
    "\n",
    "\n",
    "# Description\n",
    "\n",
    "None\n",
    "\n",
    "# Metadata\n",
    "\n",
    "- Key: 242_esg_metadata \n",
    "- Epic: Models\n",
    "- US: Estimate sign of effect\n",
    "- Task tag: #draft, #polymer, #sign-of-effect\n",
    "- Analytics reports: \n",
    "\n",
    "# Input\n",
    "\n",
    "## Table/file\n",
    "\n",
    "**Name**\n",
    "\n",
    "None\n",
    "\n",
    "**Github**\n",
    "\n",
    "- https://github.com/thomaspernet/esg_metadata/blob/master/02_data_analysis/01_model_train_evaluate/01_sign_of_effect/00_sign_of_effect_classification.md\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Connexion server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from awsPy.aws_glue import service_glue\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import seaborn as sns\n",
    "import os, shutil, json\n",
    "import sys\n",
    "import janitor\n",
    "\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent.parent.parent)\n",
    "\n",
    "\n",
    "name_credential = 'financial_dep_SO2_accessKeys.csv'\n",
    "region = 'eu-west-2'\n",
    "bucket = 'datalake-london'\n",
    "path_cred = \"{0}/creds/{1}\".format(parent_path, name_credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = region)\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = bucket, verbose = False)\n",
    "glue = service_glue.connect_glue(client = client) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "pandas_setting = True\n",
    "if pandas_setting:\n",
    "    #cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Load tables\n",
    "\n",
    "Since we load the data as a Pandas DataFrame, we want to pass the `dtypes`. We load the schema from Glue to guess the types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "db = 'esg'\n",
    "table = 'meta_analysis_esg_cfp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "dtypes = {}\n",
    "schema = (glue.get_table_information(database = db,\n",
    "                           table = table)\n",
    "          ['Table']['StorageDescriptor']['Columns']\n",
    "         )\n",
    "for key, value in enumerate(schema):\n",
    "    if value['Type'] in ['varchar(12)',\n",
    "                         'varchar(3)',\n",
    "                        'varchar(14)', 'varchar(11)']:\n",
    "        format_ = 'string'\n",
    "    elif value['Type'] in ['decimal(21,5)', 'double', 'bigint', 'int', 'float']:\n",
    "        format_ = 'float'\n",
    "    else:\n",
    "        format_ = value['Type'] \n",
    "    dtypes.update(\n",
    "        {value['Name']:format_}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "download_data = True\n",
    "filename = 'df_{}'.format(table)\n",
    "full_path_filename = 'SQL_OUTPUT_ATHENA/CSV/{}.csv'.format(filename)\n",
    "path_local = os.path.join(str(Path(path).parent.parent.parent), \n",
    "                              \"00_data_catalog/temporary_local_data\")\n",
    "df_path = os.path.join(path_local, filename + '.csv')\n",
    "if download_data:\n",
    "    \n",
    "    s3 = service_s3.connect_S3(client = client,\n",
    "                          bucket = bucket, verbose = False)\n",
    "    query = \"\"\"\n",
    "    WITH test as (\n",
    "  SELECT \n",
    "    *, concat(environmental,  social, governance) as filters\n",
    "  FROM {}.{} \n",
    "  WHERE \n",
    "    first_date_of_observations IS NOT NULL \n",
    "    and last_date_of_observations IS NOT NULL \n",
    "    and adjusted_model != 'TO_REMOVE' \n",
    ") \n",
    "SELECT \n",
    "  filters, to_remove, test.id, image, row_id_excel, row_id_google_spreadsheet,\n",
    "       table_refer, incremental_id, paper_name, publication_name,\n",
    "       rank, sjr, sjr_best_quartile, h_index, total_docs_2020,\n",
    "       total_docs_3years, total_refs, total_cites_3years,\n",
    "       citable_docs_3years, cites_doc_2years, country,\n",
    "       publication_year, publication_type, cnrs_ranking, peer_reviewed,\n",
    "       study_focused_on_social_environmental_behaviour, type_of_data,\n",
    "       first_date_of_observations,mid_year, last_date_of_observations,\n",
    "       windows, adjusted_model_name,\n",
    "       adjusted_model, dependent, adjusted_dependent, independent,\n",
    "       adjusted_independent, \n",
    "       social,\n",
    "       environmental,\n",
    "       governance,\n",
    "       financial_crisis,\n",
    "       kyoto,\n",
    "       regions,\n",
    "       study_focusing_on_developing_or_developed_countries,\n",
    "       lag,\n",
    "       interaction_term, quadratic_term, n, r2, beta,\n",
    "       sign_of_effect,\n",
    "       adjusted_t_value,\n",
    "       adjusted_standard_error,\n",
    "       target,\n",
    "       p_value_significant,\n",
    "       weight\n",
    "FROM \n",
    "  test \n",
    "  LEFT JOIN (\n",
    "    SELECT \n",
    "      id, \n",
    "      COUNT(*) as weight \n",
    "    FROM \n",
    "      test \n",
    "    GROUP BY \n",
    "      id\n",
    "  ) as c on test.id = c.id\n",
    "  WHERE filters != 'TrueTrueTrue' and filters != 'FalseFalseFalse' and sjr IS NOT NULL\n",
    "\n",
    "    \"\"\".format(db, table)\n",
    "    try:\n",
    "        df = (s3.run_query(\n",
    "            query=query,\n",
    "            database=db,\n",
    "            s3_output='SQL_OUTPUT_ATHENA',\n",
    "            filename=filename,  # Add filename to print dataframe\n",
    "            destination_key='SQL_OUTPUT_ATHENA/CSV',  #Use it temporarily\n",
    "            dtype = dtypes\n",
    "        )\n",
    "                )\n",
    "    except:\n",
    "        pass\n",
    "    s3.download_file(\n",
    "        key = full_path_filename\n",
    "    )\n",
    "    shutil.move(\n",
    "        filename + '.csv',\n",
    "        os.path.join(path_local, filename + '.csv')\n",
    "    )\n",
    "    s3.remove_file(full_path_filename)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "df.isna().sum().sort_values().loc[lambda x: x> 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "df['adjusted_model'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Save data to Google Spreadsheet\n",
    "\n",
    "Data is in [METADATA_MODEL-FINAL_DATA](https://docs.google.com/spreadsheets/d/13gpRy93l7POWGe-rKjytt7KWOcD1oSLACngTEpuqCTg/edit#gid=1219457110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade git+git://github.com/thomaspernet/GoogleDrive-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "python3"
   },
   "outputs": [],
   "source": [
    "from GoogleDrivePy.google_drive import connect_drive\n",
    "from GoogleDrivePy.google_authorization import authorization_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "python3"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(\"creds\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "s3.download_file(key = \"CREDS/Financial_dependency_pollution/creds/token.pickle\", path_local = \"creds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "python3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "auth = authorization_service.get_authorization(\n",
    "    #path_credential_gcp=os.path.join(parent_path, \"creds\", \"service.json\"),\n",
    "    path_credential_drive=os.path.join(os.getcwd(), \"creds\"),\n",
    "    verbose=False,\n",
    "    scope=['https://www.googleapis.com/auth/spreadsheets.readonly',\n",
    "           \"https://www.googleapis.com/auth/drive\"]\n",
    ")\n",
    "gd_auth = auth.authorization_drive(path_secret=os.path.join(\n",
    "    os.getcwd(), \"creds\", \"credentials.json\"))\n",
    "drive = connect_drive.drive_operations(gd_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "python3"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(os.path.join(os.getcwd(),\"creds\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "python3"
   },
   "outputs": [],
   "source": [
    "FILENAME_SPREADSHEET = \"METADATA_MODEL\"\n",
    "spreadsheet_id = drive.find_file_id(FILENAME_SPREADSHEET, to_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "python3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "path_local = os.path.join(str(Path(os.getcwd()).parent.parent.parent), \n",
    "                              \"00_data_catalog/temporary_local_data\")\n",
    "output = pd.read_csv( os.path.join(path_local, 'df_meta_analysis_esg_cfp' + '.csv'))\n",
    "drive.add_data_to_spreadsheet(\n",
    "    data =output.fillna(\"\"),\n",
    "    sheetID =spreadsheet_id,\n",
    "    sheetName = \"FINAL_DATA\",\n",
    "    detectRange = True,\n",
    "    rangeData = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## unbalanced ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "df['weight'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .loc[lambda x: x['adjusted_t_value'] <=10 ]\n",
    "    .reindex(columns = ['adjusted_t_value'])\n",
    "    .plot\n",
    "    .hist(10, figsize= (6,6))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "df['adjusted_t_value'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Validation text\n",
    "\n",
    "\"our final database includes 588 studies, divided into 51 journals, 90 titles and 87 different first authors. It is therefore important to note that, among all the studies ultimately selected for our study, 38% of the observations are concentrated in 10 papers and 10 authors\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "- includes 588 studies: CORRECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "- divided into 51 journals: It should be 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "df['publication_name'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "- 90 titles: It should be 78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "df['id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "- 87 different first authors: TO CHECK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "- 38% of the observations are concentrated in 10 papers: It should be 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    (df.groupby('id')['id'].count()/df.shape[0]).rename(\"count\")\n",
    "    .reset_index()\n",
    "    .sort_values(by = ['count'], ascending = False)\n",
    "    .assign(cum_sum = lambda x: x['count'].cumsum())\n",
    "    .reset_index()\n",
    "    .drop(columns = ['index'])\n",
    "    .head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Schema Latex table\n",
    "\n",
    "To rename a variable, please use the following template:\n",
    "\n",
    "```\n",
    "{\n",
    "    'old':'XX',\n",
    "    'new':'XX_1'\n",
    "    }\n",
    "```\n",
    "\n",
    "if you need to pass a latex format with `\\`, you need to duplicate it for instance, `\\text` becomes `\\\\text:\n",
    "\n",
    "```\n",
    "{\n",
    "    'old':'working\\_capital\\_i',\n",
    "    'new':'\\\\text{working capital}_i'\n",
    "    }\n",
    "```\n",
    "\n",
    "Then add it to the key `to_rename`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "add_to_dic = False\n",
    "if add_to_dic:\n",
    "    if os.path.exists(\"schema_table.json\"):\n",
    "        os.remove(\"schema_table.json\")\n",
    "    data = {'to_rename':[], 'to_remove':[]}\n",
    "    dic_rename = [\n",
    "        {\n",
    "        'old':'working\\_capital\\_i',\n",
    "        'new':'\\\\text{working capital}_i'\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    data['to_rename'].extend(dic_rename)\n",
    "    with open('schema_table.json', 'w') as outfile:\n",
    "        json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(parent_path, 'utils'))\n",
    "import latex.latex_beautify as lb\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge r-lmtest -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "R"
   },
   "outputs": [],
   "source": [
    "options(warn=-1)\n",
    "library(tidyverse)\n",
    "library(\"sandwich\")\n",
    "library(\"lmtest\")\n",
    "#library(lfe)\n",
    "#library(lazyeval)\n",
    "#library(nnet)\n",
    "library('progress')\n",
    "path = \"../../../utils/latex/table_golatex.R\"\n",
    "source(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "R"
   },
   "outputs": [],
   "source": [
    "%get df_path\n",
    "df_final <- read_csv(df_path) %>%\n",
    "mutate_if(is.character, as.factor) %>%\n",
    "mutate(\n",
    "    sign_of_effect = relevel(sign_of_effect, ref='NEGATIVE'),\n",
    "    adjusted_model = relevel(adjusted_model, ref='OTHER'),\n",
    "    adjusted_dependent = relevel(adjusted_dependent, ref='OTHER'),\n",
    "      id = as.factor(id),\n",
    "    governance = relevel(as.factor(governance), ref = 'NO'),\n",
    "    social = relevel(as.factor(social), ref = 'NO'),\n",
    "    environmental =relevel(as.factor(environmental), ref = 'NO'),\n",
    "    financial_crisis =relevel(as.factor(financial_crisis), ref = 'NO'),\n",
    "    kyoto =relevel(as.factor(kyoto), ref = 'NO'),\n",
    "    target =relevel(as.factor(target), ref = 'NOT_SIGNIFICANT'),\n",
    "    study_focusing_on_developing_or_developed_countries =relevel(\n",
    "        as.factor(study_focusing_on_developing_or_developed_countries), ref = 'WORLDWIDE'),\n",
    "    regions =relevel(as.factor(regions), ref = 'WORLDWIDE'),\n",
    "    cnrs_ranking =relevel(as.factor(cnrs_ranking), ref = '0'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "R"
   },
   "outputs": [],
   "source": [
    "glimpse(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "R"
   },
   "outputs": [],
   "source": [
    "transpose(df_final %>% \n",
    "    select_if(function(x) any(is.na(x))) %>% \n",
    "    summarise_each(funs(sum(is.na(.)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "R"
   },
   "source": [
    "GLM does not clustered the standard error so, we compute it by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "R"
   },
   "outputs": [],
   "source": [
    "se_robust <- function(x)\n",
    "  coeftest(x, vcov. = sandwich::sandwich\n",
    "          )[, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "R"
   },
   "source": [
    "# Table 1: Probit\n",
    "\n",
    "$$\n",
    "\\mathrm{P}\\left(\\text { Significant }_{\\mathrm{ib}}=\\mathrm{significant}\\right)=\\mathrm{\\beta}_{0} + \n",
    "\\mathrm{\\beta}_{1}\\text { ESG }_{\\mathrm{ib}}+ \n",
    "\\mathrm{\\beta}_{2}\\text { Kyoto }_{\\mathrm{i}} +\n",
    "\\mathrm{\\beta}_{3}\\text { Financial crisis }_{\\mathrm{i}} +\n",
    "\\mathrm{\\beta}_{4}\\text { Publication year }_{\\mathrm{i}} + \n",
    "\\mathrm{\\beta}_{5}\\text { windows }_{\\mathrm{i}} +\n",
    "\\mathrm{\\beta}_{6}\\text { mid-year }_{\\mathrm{i}} +\n",
    "\\mathrm{\\beta}_{7}\\text { region }_{\\mathrm{ib}}\n",
    "+\\epsilon _{\\mathrm{ib}}\n",
    "$$\n",
    "\n",
    "- robust standard error\n",
    "- Cannot compute clustered standard error if we add features without variation among the c luster (i.e `n`, or journal information)\n",
    "\n",
    "## Variable construction\n",
    "\n",
    "\n",
    "* Significant: If in the table, p-value below .1, then significant else not significant\n",
    "* The variable adjusted_independent is too imbalanced, and we are interested in only:\n",
    "  * SOCIAL\n",
    "  * ENVIRONMENTAL\n",
    "  * GOVERNANCE\n",
    "* So need to create three underlying dummy variables: rules below\n",
    "  * Source low-level variable: https://docs.google.com/spreadsheets/d/1d66_CVtWni7wmKlIMcpaoanvT2ghmjbXARiHgnLWvUw/edit#gid=146632716&range=B126\n",
    "  * SOCIAL if adjusted_independent : \n",
    "    * ENVIRONMENTAL AND SOCIAL\n",
    "    * SOCIAL\n",
    "    * CSP\n",
    "    * CSR\n",
    "    * ENVIRONMENTAL, SOCIAL and GOVERNANCE\n",
    "  * ENVIRONMENTAL if adjusted_independent :\n",
    "    * ENVIRONMENTAL\n",
    "    * ENVIRONMENTAL AND SOCIAL\n",
    "    * ENVIRONMENTAL, SOCIAL and GOVERNANCE\n",
    "  * GOVERNANCE if adjusted_independent :\n",
    "    * GOVERNANCE\n",
    "    * ENVIRONMENTAL, SOCIAL and GOVERNANCE\n",
    "- adjusted_model: https://docs.google.com/spreadsheets/d/1d66_CVtWni7wmKlIMcpaoanvT2ghmjbXARiHgnLWvUw/edit#gid=793443705&range=B34\n",
    "- adjusted_dependent: https://docs.google.com/spreadsheets/d/1d66_CVtWni7wmKlIMcpaoanvT2ghmjbXARiHgnLWvUw/edit#gid=450174628&range=B59\n",
    "- Region:\n",
    "    - AFRICA: 'Cameroon', 'Egypt', 'Libya', 'Morocco', 'Nigeria'\n",
    "    - ASIA AND PACIFIC:  'India', 'Indonesia', 'Taiwan', 'Vietnam', \n",
    "        'Australia', 'China', 'Iran', 'Malaysia', \n",
    "        'Pakistan', 'South Korea', 'Bangladesh'\n",
    "    - EUROPE: 'Spain', '20 European countries', \n",
    "        'United Kingdom', 'France', 'Germany, Italy, the Netherlands and United Kingdom', \n",
    "        'Turkey', 'UK'\n",
    "    - LATIN AMERICA: 'Latin America', 'Brazil'\n",
    "    - NORTH AMERICA: 'USA', 'US', 'U.S.', 'Canada'\n",
    "    - ELSE WORLDWIDE\n",
    "- Kyoto first_date_of_observations >= 1997 THEN TRUE ELSE FALSE ,\n",
    "- Financial crisis first_date_of_observations >= 2009 THEN TRUE ELSE FALSE \n",
    "- windows: last_date_of_observations - first_date_of_observations\n",
    "- mid-year: last_date_of_observations - (windows/2)\n",
    "\n",
    "\n",
    "## note about Probit \n",
    "\n",
    "TO estimate a probit, use `probit` link function.  For logistic regression, use `binomial`\n",
    "\n",
    "- Reason Probit instead of Logit\n",
    "    - [What is the Difference Between Logit and Probit Models?](https://tutorials.methodsconsultants.com/posts/what-is-the-difference-between-logit-and-probit-models/)\n",
    "\n",
    "Logit and probit differ in how they define $f(∗)$. The logit model uses something called the cumulative distribution function of the logistic distribution. The probit model uses something called the cumulative distribution function of the standard normal distribution to define $f(∗)$.\n",
    "\n",
    "Probit models can be generalized to account for non-constant error variances in more advanced econometric settings (known as heteroskedastic probit models)\n",
    "\n",
    "## How to read\n",
    "\n",
    "**Comparison group**\n",
    "\n",
    "- Always `OTHER`\n",
    "- Target: `SIGNIFICANT`\n",
    "- regions: `WORLDWIDE`\n",
    "- cnrs_ranking: `0`\n",
    "\n",
    "**Odd ratio**\n",
    "\n",
    "- Categorical:\n",
    "    - Keeping all other variables constant, if the analysis uses FIXED EFFECT model, there are 2.71 times more likely to stay in the NEGATIVE sign category as compared to the OTHER model category. The coefficient, however, is not significant. (Col 1)\n",
    "- Continuous:\n",
    "    - Keeping all other variables constant, if the SJR score increases one unit, there is 1.003 times more likely to stay in the POSITIVE sign category as compared to the OTHER model category y (the risk or odds is .2% higher). The coefficient is significant.\n",
    "    \n",
    "Here, OTHER means insignificant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "R"
   },
   "outputs": [],
   "source": [
    "### Baseline SJR\n",
    "t_0 <- glm(target ~ environmental\n",
    "           + adjusted_model  \n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + sjr,\n",
    "           data = df_final ,\n",
    "           binomial(link = \"probit\")\n",
    "          )\n",
    "t_0.rrr <- exp(coef(t_0))\n",
    "t_1 <- glm(target ~ social\n",
    "           + adjusted_model    \n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + sjr,\n",
    "           data = df_final , binomial(link = \"probit\"))\n",
    "t_1.rrr <- exp(coef(t_1))\n",
    "t_2 <- glm(target ~ governance\n",
    "           + adjusted_model\n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + sjr,\n",
    "           data = df_final , binomial(link = \"probit\"))\n",
    "t_2.rrr <- exp(coef(t_2))\n",
    "### Econometrics control\n",
    "t_3 <- glm(target ~ environmental\n",
    "           + adjusted_model  \n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + sjr\n",
    "           + lag\n",
    "           + interaction_term\n",
    "           + quadratic_term,\n",
    "           data = df_final ,\n",
    "           binomial(link = \"probit\")\n",
    "          )\n",
    "t_3.rrr <- exp(coef(t_3))\n",
    "t_4 <- glm(target ~ social\n",
    "           + adjusted_model    \n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + sjr\n",
    "           + lag\n",
    "           + interaction_term\n",
    "           + quadratic_term,\n",
    "           data = df_final , binomial(link = \"probit\"))\n",
    "t_4.rrr <- exp(coef(t_4))\n",
    "t_5 <- glm(target ~ governance\n",
    "           + adjusted_model\n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + sjr\n",
    "           + lag\n",
    "           + interaction_term\n",
    "           + quadratic_term,\n",
    "           data = df_final , binomial(link = \"probit\"))\n",
    "t_5.rrr <- exp(coef(t_5))\n",
    "\n",
    "list_final = list(t_0, t_1, t_2, t_3, t_4, t_5)\n",
    "list_final.rrr = list(t_0.rrr,t_1.rrr ,t_2.rrr,t_3.rrr,t_4.rrr,t_5.rrr)\n",
    "stargazer(list_final, type = \"text\", \n",
    "  se = lapply(list_final,\n",
    "              se_robust),\n",
    "          coef=list_final.rrr,\n",
    "          style = \"qje\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "R"
   },
   "outputs": [],
   "source": [
    "### Baseline SJR\n",
    "t_0 <- glm(target ~ environmental\n",
    "           + adjusted_model  \n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + cnrs_ranking,\n",
    "           data = df_final ,\n",
    "           binomial(link = \"probit\")\n",
    "          )\n",
    "t_0.rrr <- exp(coef(t_0))\n",
    "t_1 <- glm(target ~ social\n",
    "           + adjusted_model    \n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + cnrs_ranking,\n",
    "           data = df_final , binomial(link = \"probit\"))\n",
    "t_1.rrr <- exp(coef(t_1))\n",
    "t_2 <- glm(target ~ governance\n",
    "           + adjusted_model\n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + cnrs_ranking,\n",
    "           data = df_final , binomial(link = \"probit\"))\n",
    "t_2.rrr <- exp(coef(t_2))\n",
    "### Econometrics control\n",
    "t_3 <- glm(target ~ environmental\n",
    "           + adjusted_model  \n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + cnrs_ranking\n",
    "           + lag\n",
    "           + interaction_term\n",
    "           + quadratic_term,\n",
    "           data = df_final ,\n",
    "           binomial(link = \"probit\")\n",
    "          )\n",
    "t_3.rrr <- exp(coef(t_3))\n",
    "t_4 <- glm(target ~ social\n",
    "           + adjusted_model    \n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + cnrs_ranking\n",
    "           + lag\n",
    "           + interaction_term\n",
    "           + quadratic_term,\n",
    "           data = df_final , binomial(link = \"probit\"))\n",
    "t_4.rrr <- exp(coef(t_4))\n",
    "t_5 <- glm(target ~ governance\n",
    "           + adjusted_model\n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + cnrs_ranking\n",
    "           + lag\n",
    "           + interaction_term\n",
    "           + quadratic_term,\n",
    "           data = df_final , binomial(link = \"probit\"))\n",
    "t_5.rrr <- exp(coef(t_5))\n",
    "\n",
    "list_final = list(t_0, t_1, t_2, t_3, t_4, t_5)\n",
    "list_final.rrr = list(t_0.rrr,t_1.rrr ,t_2.rrr,t_3.rrr,t_4.rrr,t_5.rrr)\n",
    "stargazer(list_final, type = \"text\", \n",
    "  se = lapply(list_final,\n",
    "              se_robust),\n",
    "          coef=list_final.rrr,\n",
    "          style = \"qje\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Model OLS: \n",
    "\n",
    "$$\n",
    " \\text{T-value}=\\mathrm{\\beta}_{0} + \n",
    "\\mathrm{\\beta}_{1}\\text { ESG }_{\\mathrm{ib}}+ \n",
    "\\mathrm{\\beta}_{2}\\text { Kyoto }_{\\mathrm{i}} +\n",
    "\\mathrm{\\beta}_{3}\\text { Financial crisis }_{\\mathrm{i}} +\n",
    "\\mathrm{\\beta}_{4}\\text { Publication year }_{\\mathrm{i}} + \n",
    "\\mathrm{\\beta}_{5}\\text { windows }_{\\mathrm{i}} +\n",
    "\\mathrm{\\beta}_{6}\\text { mid-year }_{\\mathrm{i}} +\n",
    "\\mathrm{\\beta}_{7}\\text { region }_{\\mathrm{ib}}\n",
    "+\\epsilon _{\\mathrm{ib}}\n",
    "$$\n",
    "\n",
    "### Computation t-value\n",
    "\n",
    "* construct should_t_value   equals to “TO_CHECK” → if test_standard_error   = “TO_CHECK” and adjusted_model  is not PANEL or POOLED (use panel because panel use clustered/robust standard error no direct computation), then check if switch standard error and t-stat, so use column sr has t-stat and compare with critical value. If match critical value, and equals to stars  then OK, else “TO_CHECK”\n",
    "* Construct adjusted_standard_error : if test_standard_error  is OK and should_t_value  is NO_NEED_TO_CHECK then use sr , else leave blank\n",
    "* Construct **adjusted_t_value**: \n",
    "  * ⚠️ critical value (the raw data has a column for the t_value which is similar, but the variable adjusted_t_value is reconstructed based on known t_value or in case of unknown t_value then from standard error or p-value: \n",
    "    * If test_t_value is equals to TO_CHECK or OK then use t_value ← We use the value reported in the paper, not the one reconstructed\n",
    "    * ELSE if test_standard_error is equal to NO_SE and test_p_value is equal to OK then we can compute the critical value using the t-inverse function. \n",
    "      * Ex: round(T.INV(1-X114, I114) where X114 is the p-value, so we want to get the right tail. If p-value is .05, the the right tail is .95.\n",
    "    * ELSE beta / standard error \n",
    "    * Note, if critical value cannot be computed, it is because of one of the following reason\n",
    "      *  p-value is 0, then cannot compute the critical value\n",
    "      * standard error is 0, cannot divide by 0\n",
    "      * missing standard error, t-value or p-value\n",
    "      \n",
    "- Remove 10 outliers -> critical value more than 1K -> high leverage and does not represent the true data\n",
    "- Standard error robust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Model 1: No absolute value\n",
    "\n",
    "Interested in the magnitude of the t-student critical value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "R"
   },
   "outputs": [],
   "source": [
    "### Baseline SJR\n",
    "t_0 <- glm(adjusted_t_value ~ environmental\n",
    "           + adjusted_model  \n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + sjr,\n",
    "           data = df_final %>% filter(adjusted_t_value < 10),\n",
    "           family=gaussian(identity))\n",
    "\n",
    "t_1 <- glm(adjusted_t_value ~ social\n",
    "           + adjusted_model    \n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + sjr,\n",
    "           data = df_final %>% filter(adjusted_t_value < 10),\n",
    "           family=gaussian(identity))\n",
    "\n",
    "t_2 <- glm(adjusted_t_value ~ governance\n",
    "           + adjusted_model\n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + sjr,\n",
    "           data = df_final %>% filter(adjusted_t_value < 10),\n",
    "           family=gaussian(identity))\n",
    "\n",
    "### Econometrics control\n",
    "t_3 <- glm(adjusted_t_value ~ environmental\n",
    "           + adjusted_model  \n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + sjr\n",
    "           + lag\n",
    "           + interaction_term\n",
    "           + quadratic_term,\n",
    "           data = df_final %>% filter(adjusted_t_value < 10),\n",
    "           family=gaussian(identity))\n",
    "\n",
    "t_4 <- glm(adjusted_t_value ~ social\n",
    "           + adjusted_model    \n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + sjr\n",
    "           + lag\n",
    "           + interaction_term\n",
    "           + quadratic_term,\n",
    "           data = df_final %>% filter(adjusted_t_value < 10),\n",
    "           family=gaussian(identity))\n",
    "\n",
    "t_5 <- glm(adjusted_t_value ~ governance\n",
    "           + adjusted_model\n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + sjr\n",
    "           + lag\n",
    "           + interaction_term\n",
    "           + quadratic_term,\n",
    "           data = df_final %>% filter(adjusted_t_value < 10),\n",
    "           family=gaussian(identity))\n",
    "\n",
    "\n",
    "list_final = list(t_0, t_1, t_2, t_3, t_4, t_5)\n",
    "stargazer(list_final, type = \"text\", \n",
    "  se = lapply(list_final,\n",
    "              se_robust),\n",
    "          style = \"qje\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "R"
   },
   "outputs": [],
   "source": [
    "### Baseline SJR\n",
    "t_0 <- glm(adjusted_t_value ~ environmental\n",
    "           + adjusted_model  \n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + cnrs_ranking,\n",
    "           data = df_final %>% filter(adjusted_t_value < 10),\n",
    "           family=gaussian(identity))\n",
    "\n",
    "t_1 <- glm(adjusted_t_value ~ social\n",
    "           + adjusted_model    \n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + cnrs_ranking,\n",
    "           data = df_final %>% filter(adjusted_t_value < 10),\n",
    "           family=gaussian(identity))\n",
    "\n",
    "t_2 <- glm(adjusted_t_value ~ governance\n",
    "           + adjusted_model\n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + cnrs_ranking,\n",
    "           data = df_final %>% filter(adjusted_t_value < 10),\n",
    "           family=gaussian(identity))\n",
    "\n",
    "### Econometrics control\n",
    "t_3 <- glm(adjusted_t_value ~ environmental\n",
    "           + adjusted_model  \n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + cnrs_ranking\n",
    "           + lag\n",
    "           + interaction_term\n",
    "           + quadratic_term,\n",
    "           data = df_final %>% filter(adjusted_t_value < 10),\n",
    "           family=gaussian(identity))\n",
    "\n",
    "t_4 <- glm(adjusted_t_value ~ social\n",
    "           + adjusted_model    \n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + cnrs_ranking\n",
    "           + lag\n",
    "           + interaction_term\n",
    "           + quadratic_term,\n",
    "           data = df_final %>% filter(adjusted_t_value < 10),\n",
    "           family=gaussian(identity))\n",
    "\n",
    "t_5 <- glm(adjusted_t_value ~ governance\n",
    "           + adjusted_model\n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + cnrs_ranking\n",
    "           + lag\n",
    "           + interaction_term\n",
    "           + quadratic_term,\n",
    "           data = df_final %>% filter(adjusted_t_value < 10),\n",
    "           family=gaussian(identity))\n",
    "\n",
    "\n",
    "list_final = list(t_0, t_1, t_2, t_3, t_4, t_5)\n",
    "stargazer(list_final, type = \"text\", \n",
    "  se = lapply(list_final,\n",
    "              se_robust),\n",
    "          style = \"qje\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "R"
   },
   "source": [
    "Model 2: absolute value\n",
    "\n",
    "Interested in the factors leading to larger t-student critical value, hence significant coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "R"
   },
   "outputs": [],
   "source": [
    "### Baseline SJR\n",
    "t_0 <- glm(abs(adjusted_t_value) ~ environmental\n",
    "           + adjusted_model  \n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + sjr,\n",
    "           data = df_final %>% filter(adjusted_t_value < 10),\n",
    "           family=gaussian(identity))\n",
    "\n",
    "t_1 <- glm(abs(adjusted_t_value) ~ social\n",
    "           + adjusted_model    \n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + sjr,\n",
    "           data = df_final %>% filter(adjusted_t_value < 10),\n",
    "           family=gaussian(identity))\n",
    "\n",
    "t_2 <- glm(abs(adjusted_t_value) ~ governance\n",
    "           + adjusted_model\n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + sjr,\n",
    "           data = df_final %>% filter(adjusted_t_value < 10),\n",
    "           family=gaussian(identity))\n",
    "\n",
    "### Econometrics control\n",
    "t_3 <- glm(abs(adjusted_t_value) ~ environmental\n",
    "           + adjusted_model  \n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + sjr\n",
    "           + lag\n",
    "           + interaction_term\n",
    "           + quadratic_term,\n",
    "           data = df_final %>% filter(adjusted_t_value < 10),\n",
    "           family=gaussian(identity))\n",
    "\n",
    "t_4 <- glm(abs(adjusted_t_value) ~ social\n",
    "           + adjusted_model    \n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + sjr\n",
    "           + lag\n",
    "           + interaction_term\n",
    "           + quadratic_term,\n",
    "           data = df_final %>% filter(adjusted_t_value < 10),\n",
    "           family=gaussian(identity))\n",
    "\n",
    "t_5 <- glm(abs(adjusted_t_value) ~ governance\n",
    "           + adjusted_model\n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + sjr\n",
    "           + lag\n",
    "           + interaction_term\n",
    "           + quadratic_term,\n",
    "           data = df_final %>% filter(adjusted_t_value < 10),\n",
    "           family=gaussian(identity))\n",
    "\n",
    "\n",
    "list_final = list(t_0, t_1, t_2, t_3, t_4, t_5)\n",
    "stargazer(list_final, type = \"text\", \n",
    "  se = lapply(list_final,\n",
    "              se_robust),\n",
    "          style = \"qje\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "R"
   },
   "outputs": [],
   "source": [
    "### Baseline SJR\n",
    "t_0 <- glm(abs(adjusted_t_value) ~ environmental\n",
    "           + adjusted_model  \n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + cnrs_ranking,\n",
    "           data = df_final %>% filter(adjusted_t_value < 10),\n",
    "           family=gaussian(identity))\n",
    "\n",
    "t_1 <- glm(abs(adjusted_t_value) ~ social\n",
    "           + adjusted_model    \n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + cnrs_ranking,\n",
    "           data = df_final %>% filter(adjusted_t_value < 10),\n",
    "           family=gaussian(identity))\n",
    "\n",
    "t_2 <- glm(abs(adjusted_t_value) ~ governance\n",
    "           + adjusted_model\n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + cnrs_ranking,\n",
    "           data = df_final %>% filter(adjusted_t_value < 10),\n",
    "           family=gaussian(identity))\n",
    "\n",
    "### Econometrics control\n",
    "t_3 <- glm(abs(adjusted_t_value) ~ environmental\n",
    "           + adjusted_model  \n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + cnrs_ranking\n",
    "           + lag\n",
    "           + interaction_term\n",
    "           + quadratic_term,\n",
    "           data = df_final %>% filter(adjusted_t_value < 10),\n",
    "           family=gaussian(identity))\n",
    "\n",
    "t_4 <- glm(abs(adjusted_t_value) ~ social\n",
    "           + adjusted_model    \n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + cnrs_ranking\n",
    "           + lag\n",
    "           + interaction_term\n",
    "           + quadratic_term,\n",
    "           data = df_final %>% filter(adjusted_t_value < 10),\n",
    "           family=gaussian(identity))\n",
    "\n",
    "t_5 <- glm(abs(adjusted_t_value) ~ governance\n",
    "           + adjusted_model\n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + cnrs_ranking\n",
    "           + lag\n",
    "           + interaction_term\n",
    "           + quadratic_term,\n",
    "           data = df_final %>% filter(adjusted_t_value < 10),\n",
    "           family=gaussian(identity))\n",
    "\n",
    "\n",
    "list_final = list(t_0, t_1, t_2, t_3, t_4, t_5)\n",
    "stargazer(list_final, type = \"text\", \n",
    "  se = lapply(list_final,\n",
    "              se_robust),\n",
    "          style = \"qje\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "R"
   },
   "source": [
    "# Statistics\n",
    "\n",
    "## Target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            (\n",
    "                                df.groupby(\"environmental\")\n",
    "                                .agg({\"target\": \"value_counts\"})\n",
    "                                .unstack(0)\n",
    "                            ).rename(columns={\"target\": \"environment\"}),\n",
    "                            (\n",
    "                                df.groupby(\"social\")\n",
    "                                .agg({\"target\": \"value_counts\"})\n",
    "                                .unstack(0)\n",
    "                            ).rename(columns={\"target\": \"social\"}),\n",
    "                            (\n",
    "                                df.groupby(\"governance\")\n",
    "                                .agg({\"target\": \"value_counts\"})\n",
    "                                .unstack(0)\n",
    "                            ).rename(columns={\"target\": \"governance\"}),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    .T.reset_index()\n",
    "                    .rename(columns={\"environmental\": \"is_dummy\", \"level_0\": \"origin\"})\n",
    "                    .set_index([\"origin\", \"is_dummy\"])\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"SIGNIFICANT\")] / x.sum(axis=1)\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            (\n",
    "                                df.groupby([\"environmental\", \"target\"])\n",
    "                                .agg({\"id\": \"nunique\"})\n",
    "                                .rename(columns={\"id\": \"environment\"})\n",
    "                                .unstack(0)\n",
    "                            ),\n",
    "                            (\n",
    "                                df.groupby([\"social\", \"target\"])\n",
    "                                .agg({\"id\": \"nunique\"})\n",
    "                                .rename(columns={\"id\": \"social\"})\n",
    "                                .unstack(0)\n",
    "                            ),\n",
    "                            (\n",
    "                                df.groupby([\"governance\", \"target\"])\n",
    "                                .agg({\"id\": \"nunique\"})\n",
    "                                .rename(columns={\"id\": \"governance\"})\n",
    "                                .unstack(0)\n",
    "                            ),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    .T.reset_index()\n",
    "                    .rename(columns={\"environmental\": \"is_dummy\", \"level_0\": \"origin\"})\n",
    "                    .set_index([\"origin\", \"is_dummy\"])\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"SIGNIFICANT\")] / x.sum(axis=1)\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"paper count\"],\n",
    "        ),\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"adjusted_model\")\n",
    "                    .agg({\"target\": \"value_counts\"})\n",
    "                    .unstack(-1)\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"target\", \"SIGNIFICANT\")]\n",
    "                        / x.sum(axis=1)\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby([\"adjusted_model\", \"target\"])\n",
    "                    .agg({\"id\": \"nunique\"})\n",
    "                    .rename(columns={\"id\": \"adjusted_model\"})\n",
    "                    .unstack(-1)\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"adjusted_model\", \"SIGNIFICANT\")]\n",
    "                        / x.sum(axis=1)\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"paper count\"],\n",
    "        ),\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pd.concat(\n",
    "        [\n",
    "            (df.groupby(\"kyoto\").agg({\"target\": \"value_counts\"}).unstack(0)).rename(columns = {'target':'kyoto'}),\n",
    "            (df.groupby(\"financial_crisis\").agg({\"target\": \"value_counts\"}).unstack(0)).rename(columns = {'target':'financial_crisis'}),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    .T\n",
    "    .reset_index()\n",
    "    .rename(columns = {'kyoto':'is_dummy', 'level_0':'origin'})\n",
    "    .set_index(['origin','is_dummy'])\n",
    "    .assign(pct_significant = lambda x: x[('SIGNIFICANT')]/x.sum(axis= 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            (\n",
    "                                df.groupby(\"kyoto\")\n",
    "                                .agg({\"target\": \"value_counts\"})\n",
    "                                .unstack(0)\n",
    "                            ).rename(columns={\"target\": \"kyoto\"}),\n",
    "                            (\n",
    "                                df.groupby(\"financial_crisis\")\n",
    "                                .agg({\"target\": \"value_counts\"})\n",
    "                                .unstack(0)\n",
    "                            ).rename(columns={\"target\": \"financial_crisis\"}),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    .T.reset_index()\n",
    "                    .rename(columns={\"kyoto\": \"is_dummy\", \"level_0\": \"origin\"})\n",
    "                    .set_index([\"origin\", \"is_dummy\"])\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"SIGNIFICANT\")] / x.sum(axis=1)\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            (\n",
    "                                df.groupby([\"kyoto\", \"target\"])\n",
    "                                .agg({\"id\": \"nunique\"})\n",
    "                                .rename(columns={\"id\": \"kyoto\"})\n",
    "                                .unstack(0)\n",
    "                            ),\n",
    "                            (\n",
    "                                df.groupby([\"financial_crisis\", \"target\"])\n",
    "                                .agg({\"id\": \"nunique\"})\n",
    "                                .rename(columns={\"id\": \"financial_crisis\"})\n",
    "                                .unstack(0)\n",
    "                            ),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    .T.reset_index()\n",
    "                    .rename(columns={\"kyoto\": \"is_dummy\", \"level_0\": \"origin\"})\n",
    "                    .set_index([\"origin\", \"is_dummy\"])\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"SIGNIFICANT\")] / x.sum(axis=1)\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"paper count\"],\n",
    "        ),\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "R"
   },
   "source": [
    "## Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .groupby('target')\n",
    "    .agg(\n",
    "    {\n",
    "        'windows':'describe'\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "R"
   },
   "source": [
    "- lag\n",
    "- interaction_term\n",
    "- quadratic_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pd.concat(\n",
    "        [\n",
    "            (df.groupby(\"lag\").agg({\"target\": \"value_counts\"}).unstack(0)).rename(columns = {'target':'lag'}),\n",
    "            (df.groupby(\"interaction_term\").agg({\"target\": \"value_counts\"}).unstack(0)).rename(columns = {'target':'interaction_term'}),\n",
    "            (df.groupby(\"quadratic_term\").agg({\"target\": \"value_counts\"}).unstack(0)).rename(columns = {'target':'quadratic_term'}),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    .T\n",
    "    .reset_index()\n",
    "    .rename(columns = {'lag':'is_dummy', 'level_0':'origin'})\n",
    "    .set_index(['origin','is_dummy'])\n",
    "    .assign(pct_significant = lambda x: x[('SIGNIFICANT')]/x.sum(axis= 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            (\n",
    "                                df.groupby(\"lag\")\n",
    "                                .agg({\"target\": \"value_counts\"})\n",
    "                                .unstack(0)\n",
    "                            ).rename(columns={\"target\": \"lag\"}),\n",
    "                            (\n",
    "                                df.groupby(\"interaction_term\")\n",
    "                                .agg({\"target\": \"value_counts\"})\n",
    "                                .unstack(0)\n",
    "                            ).rename(columns={\"target\": \"interaction_term\"}),\n",
    "                            (\n",
    "                                df.groupby(\"quadratic_term\")\n",
    "                                .agg({\"target\": \"value_counts\"})\n",
    "                                .unstack(0)\n",
    "                            ).rename(columns={\"target\": \"quadratic_term\"}),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    .T.reset_index()\n",
    "                    .rename(columns={\"lag\": \"is_dummy\", \"level_0\": \"origin\"})\n",
    "                    .set_index([\"origin\", \"is_dummy\"])\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"SIGNIFICANT\")] / x.sum(axis=1)\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            (\n",
    "                                df.groupby([\"lag\", \"target\"])\n",
    "                                .agg({\"id\": \"nunique\"})\n",
    "                                .rename(columns={\"id\": \"lag\"})\n",
    "                                .unstack(0)\n",
    "                            ),\n",
    "                            (\n",
    "                                df.groupby([\"interaction_term\", \"target\"])\n",
    "                                .agg({\"id\": \"nunique\"})\n",
    "                                .rename(columns={\"id\": \"interaction_term\"})\n",
    "                                .unstack(0)\n",
    "                            ),\n",
    "                            (\n",
    "                                df.groupby([\"quadratic_term\", \"target\"])\n",
    "                                .agg({\"id\": \"nunique\"})\n",
    "                                .rename(columns={\"id\": \"quadratic_term\"})\n",
    "                                .unstack(0)\n",
    "                            ),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    .T.reset_index()\n",
    "                    .rename(columns={\"lag\": \"is_dummy\", \"level_0\": \"origin\"})\n",
    "                    .set_index([\"origin\", \"is_dummy\"])\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"SIGNIFICANT\")] / x.sum(axis=1)\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"paper count\"],\n",
    "        ),\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "R"
   },
   "source": [
    "##  Region\n",
    "\n",
    "- regions\n",
    "- study_focusing_on_developing_or_developed_countries\n",
    "\n",
    "Comparison group: \"WORLDWIDE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"regions\")\n",
    "                    .agg({\"target\": \"value_counts\"})\n",
    "                    .unstack(-1)\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"target\", \"SIGNIFICANT\")]\n",
    "                        / x.sum(axis=1)\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby([\"regions\", \"target\"])\n",
    "                    .agg({\"id\": \"nunique\"})\n",
    "                    .rename(columns={\"id\": \"regions\"})\n",
    "                    .unstack(-1)\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"regions\", \"SIGNIFICANT\")]\n",
    "                        / x.sum(axis=1)\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"paper count\"],\n",
    "        ),\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "R"
   },
   "source": [
    "## Journal\n",
    "\n",
    "- sjr \n",
    "- sjr_best_quartile: Q1\n",
    "- cnrs_ranking: 0\n",
    "- h_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .groupby('target')\n",
    "    .agg(\n",
    "    {\n",
    "        'sjr':'describe'\n",
    "    })\n",
    "    #.unstack(-1)\n",
    "    #.assign(pct_significant = lambda x: x[('target','SIGNIFICANT')]/x.sum(axis= 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"cnrs_ranking\")\n",
    "                    .agg({\"target\": \"value_counts\"})\n",
    "                    .unstack(-1)\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"target\", \"SIGNIFICANT\")]\n",
    "                        / x.sum(axis=1)\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby([\"cnrs_ranking\", \"target\"])\n",
    "                    .agg({\"id\": \"nunique\"})\n",
    "                    .rename(columns={\"id\": \"cnrs_ranking\"})\n",
    "                    .unstack(-1)\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"cnrs_ranking\", \"SIGNIFICANT\")]\n",
    "                        / x.sum(axis=1)\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"paper count\"],\n",
    "        ),\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .groupby('target')\n",
    "    .agg(\n",
    "    {\n",
    "        'h_index':'describe'\n",
    "    })\n",
    "    #.unstack(-1)\n",
    "    #.assign(pct_significant = lambda x: x[('target','SIGNIFICANT')]/x.sum(axis= 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Generate reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "python3",
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "import os, time, shutil, urllib, ipykernel, json\n",
    "from pathlib import Path\n",
    "from notebook import notebookapp\n",
    "import sys\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent.parent.parent)\n",
    "sys.path.append(os.path.join(parent_path, 'utils'))\n",
    "import make_toc\n",
    "import create_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "python3"
   },
   "outputs": [],
   "source": [
    "name_json = 'parameters_ETL_esg_metadata.json'\n",
    "path_json = os.path.join(str(Path(path).parent.parent), 'utils',name_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "python3",
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "create_report.create_report(extension = \"html\", keep_code = True, notebookname = \"00_sign_of_effect_classification.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "python3"
   },
   "outputs": [],
   "source": [
    "### Update TOC in Github\n",
    "for p in [parent_path,\n",
    "          str(Path(path).parent),\n",
    "          #os.path.join(str(Path(path).parent), \"00_download_data_from\"),\n",
    "          #os.path.join(str(Path(path).parent.parent), \"02_data_analysis\"),\n",
    "          #os.path.join(str(Path(path).parent.parent), \"02_data_analysis\", \"00_statistical_exploration\"),\n",
    "          #os.path.join(str(Path(path).parent.parent), \"02_data_analysis\", \"01_model_estimation\"),\n",
    "         ]:\n",
    "    try:\n",
    "        os.remove(os.path.join(p, 'README.md'))\n",
    "    except:\n",
    "        pass\n",
    "    path_parameter = os.path.join(parent_path,'utils', name_json)\n",
    "    md_lines =  make_toc.create_index(cwd = p, path_parameter = path_parameter)\n",
    "    md_out_fn = os.path.join(p,'README.md')\n",
    "    \n",
    "    if p == parent_path:\n",
    "    \n",
    "        make_toc.replace_index(md_out_fn, md_lines, Header = os.path.basename(p).replace('_', ' '), add_description = True, path_parameter = path_parameter)\n",
    "    else:\n",
    "        make_toc.replace_index(md_out_fn, md_lines, Header = os.path.basename(p).replace('_', ' '), add_description = False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "nteract": {
   "version": "0.28.0"
  },
  "sos": {
   "kernels": [
    [
     "R",
     "ir",
     "R",
     "#DCDCDA",
     "r"
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ],
    [
     "python3",
     "python3",
     "python",
     "",
     {
      "name": "ipython",
      "version": 3
     }
    ]
   ],
   "version": "0.20.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
