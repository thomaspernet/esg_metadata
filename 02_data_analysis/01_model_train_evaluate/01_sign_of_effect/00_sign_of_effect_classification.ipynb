{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# US Name\n",
    "Model estimate Estimate sign of effect\n",
    "\n",
    "\n",
    "# Description\n",
    "\n",
    "None\n",
    "\n",
    "# Metadata\n",
    "\n",
    "- Key: 242_esg_metadata \n",
    "- Epic: Models\n",
    "- US: Estimate sign of effect\n",
    "- Task tag: #draft, #polymer, #sign-of-effect\n",
    "- Analytics reports: \n",
    "\n",
    "# Input\n",
    "\n",
    "## Table/file\n",
    "\n",
    "**Name**\n",
    "\n",
    "None\n",
    "\n",
    "**Github**\n",
    "\n",
    "- https://github.com/thomaspernet/esg_metadata/blob/master/02_data_analysis/01_model_train_evaluate/01_sign_of_effect/00_sign_of_effect_classification.md\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Connexion server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from awsPy.aws_glue import service_glue\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os, shutil, json\n",
    "import sys\n",
    "import janitor\n",
    "\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent.parent.parent)\n",
    "\n",
    "\n",
    "name_credential = 'financial_dep_SO2_accessKeys.csv'\n",
    "region = 'eu-west-2'\n",
    "bucket = 'datalake-london'\n",
    "path_cred = \"{0}/creds/{1}\".format(parent_path, name_credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = region)\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = bucket, verbose = False)\n",
    "glue = service_glue.connect_glue(client = client) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "pandas_setting = True\n",
    "if pandas_setting:\n",
    "    #cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Load tables\n",
    "\n",
    "Since we load the data as a Pandas DataFrame, we want to pass the `dtypes`. We load the schema from Glue to guess the types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "db = 'esg'\n",
    "table = 'meta_analysis_esg_cfp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "dtypes = {}\n",
    "schema = (glue.get_table_information(database = db,\n",
    "                           table = table)\n",
    "          ['Table']['StorageDescriptor']['Columns']\n",
    "         )\n",
    "for key, value in enumerate(schema):\n",
    "    if value['Type'] in ['varchar(12)',\n",
    "                         'varchar(3)',\n",
    "                        'varchar(14)', 'varchar(11)']:\n",
    "        format_ = 'string'\n",
    "    elif value['Type'] in ['decimal(21,5)', 'double', 'bigint', 'int', 'float']:\n",
    "        format_ = 'float'\n",
    "    else:\n",
    "        format_ = value['Type'] \n",
    "    dtypes.update(\n",
    "        {value['Name']:format_}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "download_data = True\n",
    "filename = \"df_{}\".format(table)\n",
    "full_path_filename = \"SQL_OUTPUT_ATHENA/CSV/{}.csv\".format(filename)\n",
    "path_local = os.path.join(\n",
    "    str(Path(path).parent.parent.parent), \"00_data_catalog/temporary_local_data\"\n",
    ")\n",
    "df_path = os.path.join(path_local, filename + \".csv\")\n",
    "if download_data:\n",
    "\n",
    "    s3 = service_s3.connect_S3(client=client, bucket=bucket, verbose=False)\n",
    "    query = \"\"\"\n",
    "    WITH test as (\n",
    "  SELECT \n",
    "    *, concat(environmental,  social, governance) as filters\n",
    "  FROM esg.meta_analysis_esg_cfp\n",
    "  WHERE \n",
    "    first_date_of_observations IS NOT NULL \n",
    "    and last_date_of_observations IS NOT NULL \n",
    "    and adjusted_model != 'TO_REMOVE' \n",
    ") \n",
    "SELECT \n",
    "  filters,\n",
    " paperid,\n",
    " nb_authors,\n",
    " reference_count,\n",
    " citation_count,\n",
    " influential_citation_count,\n",
    "  CASE WHEN is_open_access = TRUE THEN 'YES' ELSE 'NO' END AS is_open_access,\n",
    " total_paper,\n",
    " esg,\n",
    " pct_esg,\n",
    " test.id_source,\n",
    " female,\n",
    " male,\n",
    " unknown,\n",
    " pct_female,\n",
    " drive_url,\n",
    " image,\n",
    " row_id_google_spreadsheet,\n",
    " table_refer,\n",
    " adjusted_model_name,\n",
    " adjusted_model,\n",
    " adjusted_dependent,\n",
    " independent,\n",
    " adjusted_independent,\n",
    " social,\n",
    " environmental,\n",
    " governance,\n",
    " lag,\n",
    " interaction_term,\n",
    " quadratic_term,\n",
    " n,\n",
    " target,\n",
    " adjusted_standard_error,\n",
    " adjusted_t_value,\n",
    " paper_name,\n",
    " first_date_of_observations,\n",
    " last_date_of_observations,\n",
    " csr_20_categories,\n",
    " kyoto,\n",
    " financial_crisis,\n",
    " windows,\n",
    " mid_year,\n",
    " regions,\n",
    " providers,\n",
    " publication_year,\n",
    " publication_name,\n",
    " rank_digit,\n",
    " CASE WHEN cluster_w_emb = 0 THEN 'CLUSTER_0'\n",
    "       WHEN cluster_w_emb = 1 THEN 'CLUSTER_1'\n",
    "       ELSE 'CLUSTER_2' END AS cluster_w_emb,\n",
    " sentiment,\n",
    " lenght,\n",
    " adj,\n",
    " noun,\n",
    " verb,\n",
    " size_abstract,\n",
    " pct_adj,\n",
    " pct_noun,\n",
    " pct_verb,\n",
    " rank,\n",
    " sjr,\n",
    " region_journal,\n",
    " weight\n",
    "FROM \n",
    "  test \n",
    "  LEFT JOIN (\n",
    "    SELECT \n",
    "      id_source, \n",
    "      COUNT(*) as weight \n",
    "    FROM \n",
    "      test \n",
    "    GROUP BY \n",
    "      id_source\n",
    "  ) as c on test.id_source = c.id_source\n",
    "  WHERE filters != 'TrueTrueTrue' and filters != 'FalseFalseFalse' and regions != 'ARAB WORLD'\n",
    "    \"\"\".format(\n",
    "        db, table\n",
    "    )\n",
    "    try:\n",
    "        df = (s3.run_query(\n",
    "            query=query,\n",
    "            database=db,\n",
    "            s3_output=\"SQL_OUTPUT_ATHENA\",\n",
    "            filename=filename,  # Add filename to print dataframe\n",
    "            destination_key=\"SQL_OUTPUT_ATHENA/CSV\",  # Use it temporarily\n",
    "            dtype=dtypes,\n",
    "        ).assign(\n",
    "            d_rank_digit=lambda x: np.where(\n",
    "                x[\"rank_digit\"].isin([\"1\"]), \"rank_1\", \"rank_2345\"\n",
    "            ),\n",
    "            publication_year_int=lambda x: pd.factorize(x[\"publication_year\"])[0],\n",
    "            interaction_term = lambda x: x['interaction_term'].str.strip()\n",
    "        ))\n",
    "    except:\n",
    "        pass\n",
    "(df.to_csv(os.path.join(path_local, \"df_meta_analysis_esg_cfp\" + \".csv\")))\n",
    "df = pd.read_csv(os.path.join(path_local, \"df_meta_analysis_esg_cfp\" + \".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Save data to Google Spreadsheet\n",
    "\n",
    "Data is in [METADATA_MODEL-FINAL_DATA](https://docs.google.com/spreadsheets/d/13gpRy93l7POWGe-rKjytt7KWOcD1oSLACngTEpuqCTg/edit#gid=1219457110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade git+git://github.com/thomaspernet/GoogleDrive-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(os.path.join(os.getcwd(),\"creds\"))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "s3.download_file(key = \"CREDS/Financial_dependency_pollution/creds/token.pickle\",\n",
    "                     path_local = \"creds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "python3"
   },
   "outputs": [],
   "source": [
    "from GoogleDrivePy.google_drive import connect_drive\n",
    "from GoogleDrivePy.google_authorization import authorization_service\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "auth = authorization_service.get_authorization(\n",
    "        path_credential_gcp=os.path.join(os.getcwd(), \"creds\", \"service.json\"),\n",
    "        path_credential_drive=os.path.join(os.getcwd(), \"creds\"),\n",
    "        verbose=False,\n",
    "        scope=['https://www.googleapis.com/auth/spreadsheets.readonly',\n",
    "               \"https://www.googleapis.com/auth/drive\"]\n",
    "    )\n",
    "gd_auth = auth.authorization_drive(path_secret=os.path.join(\n",
    "        os.getcwd(), \"creds\", \"credentials.json\"))\n",
    "drive = connect_drive.drive_operations(gd_auth)\n",
    "shutil.rmtree(os.path.join(os.getcwd(),\"creds\"))\n",
    "\n",
    "move_g_spreadsheet = False\n",
    "if move_g_spreadsheet:\n",
    "    FILENAME_SPREADSHEET = \"METADATA_MODEL\"\n",
    "    spreadsheet_id = drive.find_file_id(FILENAME_SPREADSHEET, to_print=False)\n",
    "\n",
    "    path_local = os.path.join(str(Path(os.getcwd()).parent.parent.parent), \n",
    "                                  \"00_data_catalog/temporary_local_data\")\n",
    "    output = pd.read_csv( os.path.join(path_local, 'df_meta_analysis_esg_cfp' + '.csv'))\n",
    "    drive.add_data_to_spreadsheet(\n",
    "        data =output.fillna(\"\"),\n",
    "        sheetID =spreadsheet_id,\n",
    "        sheetName = \"FINAL_DATA\",\n",
    "        detectRange = True,\n",
    "        rangeData = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "python3"
   },
   "source": [
    "# Statisitcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "python3"
   },
   "source": [
    "## Basic information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "python3"
   },
   "source": [
    "- Number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "- Number of Journals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "df['publication_name'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "- Number of publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "df['paperid'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "- Number of Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "python3"
   },
   "outputs": [],
   "source": [
    "FILENAME_SPREADSHEET = \"AUTHOR_SEMANTIC_GOOGLE\"\n",
    "spreadsheet_id = drive.find_file_id(FILENAME_SPREADSHEET, to_print=False)\n",
    "df_author = (\n",
    "    drive.download_data_from_spreadsheet(\n",
    "    sheetID = spreadsheet_id,\n",
    "    sheetName = \"Sheet1\",\n",
    "    to_dataframe = True)\n",
    "    .to_csv('temp_author.csv', index= False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pd.read_csv('temp_author.csv')\n",
    "    .loc[lambda x: x['paperId'].isin(df['paperid'].unique())]\n",
    "    .reindex(columns = ['name'])\n",
    "    .drop_duplicates()\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "- Number of papers per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pd.read_csv('temp_author.csv')\n",
    "    .loc[lambda x: x['paperId'].isin(df['paperid'].unique())]\n",
    "    .groupby('name')\n",
    "    .agg(\n",
    "    {\n",
    "        'paperId':'nunique'\n",
    "    })\n",
    "    .sort_values(by = ['paperId'])\n",
    "    .groupby('paperId')\n",
    "    .agg(\n",
    "        {\n",
    "            'paperId':'count'\n",
    "        }\n",
    "    )\n",
    "    .rename(columns = {'paperId':'count'})\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "- unbalanced ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .reindex(columns = ['weight'])\n",
    "    .plot\n",
    "    .hist(5, figsize= (6,6))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    (df.groupby('id_source')['id_source'].count()/df.shape[0]).rename(\"count\")\n",
    "    .reset_index()\n",
    "    .sort_values(by = ['count'], ascending = False)\n",
    "    .assign(cum_sum = lambda x: x['count'].cumsum())\n",
    "    .reset_index()\n",
    "    .drop(columns = ['index', 'count', 'id_source'])\n",
    "    .plot\n",
    "    .line(title = \"cumulated number of observations per paper\",figsize= (6,6))\n",
    "    \n",
    "    #.head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    #.loc[lambda x: x['environmental'].isin(['YES'])]\n",
    "    .groupby('adjusted_model')['adjusted_model_name']\n",
    "    .value_counts()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .loc[lambda x: x['environmental'].isin(['YES'])]\n",
    "    .groupby('environmental')['independent']\n",
    "    .value_counts()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "python3"
   },
   "source": [
    "## Statistic baseline\n",
    "\n",
    "- environmental \n",
    "- social \n",
    "- governance\n",
    "- adjusted_model  \n",
    "- kyoto \n",
    "- financial_crisis\n",
    "- publication_year\n",
    "- windows\n",
    "- mid_year\n",
    "- regions\n",
    "- sjr\n",
    "- is_open_access\n",
    "- region_journal\n",
    "- providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "for v in [\n",
    "    \"target\",\n",
    "    \"environmental\",\n",
    "    \"social\",\n",
    "    \"governance\",\n",
    "    \"adjusted_model\",\n",
    "    \"kyoto\",\n",
    "    \"financial_crisis\",\n",
    "    \"publication_year\",\n",
    "    \"regions\",\n",
    "    \"is_open_access\",\n",
    "    \"region_journal\",\n",
    "    \"providers\",\n",
    "]:\n",
    "    print(\"\\n\\nDisplay variable: {}\\n\\n\".format(v))\n",
    "    display(\n",
    "        pd.concat([df[v].value_counts(), df[v].value_counts(normalize=True)], axis=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "for v in ['publication_year', \"windows\", \"mid_year\", \"sjr\"]:\n",
    "    #print(\"\\n\\nDisplay variable: {}\\n\\n\".format(v))\n",
    "    (\n",
    "        df\n",
    "        .reindex(columns = [v])\n",
    "        .plot\n",
    "        .hist(10, figsize= (6,6), title = \"{} From {} to {}\".format(\n",
    "            v,\n",
    "            df[v].min(),\n",
    "            df[v].max()\n",
    "        ))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Distribution baseline feature with target\n",
    "\n",
    "\"adjusted_model\",  \n",
    "\"kyoto\" ,\n",
    "\"financial_crisis\",\n",
    "\"publication_year\",\n",
    "\"windows\",\n",
    "\"mid_year\",\n",
    "\"regions\",\n",
    "\"sjr\",\n",
    "\"is_open_access\",\n",
    "\"region_journal\",\n",
    "\"providers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### environmental, social, governance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            (\n",
    "                                df.groupby(\"environmental\")\n",
    "                                .agg({\"target\": \"value_counts\"})\n",
    "                                .unstack(0)\n",
    "                            ).rename(columns={\"target\": \"environment\"}),\n",
    "                            (\n",
    "                                df.groupby(\"social\")\n",
    "                                .agg({\"target\": \"value_counts\"})\n",
    "                                .unstack(0)\n",
    "                            ).rename(columns={\"target\": \"social\"}),\n",
    "                            (\n",
    "                                df.groupby(\"governance\")\n",
    "                                .agg({\"target\": \"value_counts\"})\n",
    "                                .unstack(0)\n",
    "                            ).rename(columns={\"target\": \"governance\"}),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    .T.reset_index()\n",
    "                    .rename(columns={\"environmental\": \"is_dummy\", \"level_0\": \"origin\"})\n",
    "                    .set_index([\"origin\", \"is_dummy\"])\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"SIGNIFICANT\")] / x.sum(axis=1),\n",
    "                        total = lambda x: x['NOT_SIGNIFICANT'] + x['SIGNIFICANT'],\n",
    "                        pct_total = lambda x: x['total']/x.groupby(['origin'])['total'].transform('sum')\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            (\n",
    "                                df.groupby([\"environmental\", \"target\"])\n",
    "                                .agg({\"id_source\": \"nunique\"})\n",
    "                                .rename(columns={\"id_source\": \"environment\"})\n",
    "                                .unstack(0)\n",
    "                            ),\n",
    "                            (\n",
    "                                df.groupby([\"social\", \"target\"])\n",
    "                                .agg({\"id_source\": \"nunique\"})\n",
    "                                .rename(columns={\"id_source\": \"social\"})\n",
    "                                .unstack(0)\n",
    "                            ),\n",
    "                            (\n",
    "                                df.groupby([\"governance\", \"target\"])\n",
    "                                .agg({\"id_source\": \"nunique\"})\n",
    "                                .rename(columns={\"id_source\": \"governance\"})\n",
    "                                .unstack(0)\n",
    "                            ),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    .T.reset_index()\n",
    "                    .rename(columns={\"environmental\": \"is_dummy\", \"level_0\": \"origin\"})\n",
    "                    .set_index([\"origin\", \"is_dummy\"])\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"SIGNIFICANT\")] / x.sum(axis=1),\n",
    "                        total = lambda x: x['NOT_SIGNIFICANT'] + x['SIGNIFICANT'],\n",
    "                        pct_total = lambda x: x['total']/x.groupby(['origin'])['total'].transform('sum')\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"paper count\"],\n",
    "        ),\n",
    "    ],\n",
    "    axis=1,\n",
    ").style.format(\"{:,.2%}\", subset = [\n",
    "    ('count','pct_significant'),\n",
    "    ('count','pct_total'),\n",
    "    ('paper count','pct_significant'),\n",
    "    ('paper count','pct_total'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### adjusted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"adjusted_model\")\n",
    "                    .agg({\"adjusted_model\": \"count\"})\n",
    "                    .rename(columns={\"adjusted_model\": \"count\"})\n",
    "                    .assign(pct=lambda x: x[\"count\"] / np.sum(x[\"count\"]))\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count raw\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"adjusted_model\")\n",
    "                    .agg({\"target\": \"value_counts\"})\n",
    "                    .unstack(0)\n",
    "                    .rename(columns={\"target\": \"adjusted_model\"})\n",
    "                    .droplevel(axis=1, level=0)\n",
    "                    .T\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count\"],\n",
    "        ),\n",
    "         pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"adjusted_model\")\n",
    "                    .agg({\"id_source\": \"nunique\"})\n",
    "                    .rename(columns={\"id_source\": \"count\"})\n",
    "                    .assign(pct=lambda x: x[\"count\"] / np.sum(x[\"count\"]))\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count paper raw\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                \n",
    "                (\n",
    "                    df.groupby([\"adjusted_model\", \"target\"])\n",
    "                    .agg({\"id_source\": \"nunique\"})\n",
    "                    .rename(columns={\"id_source\": \"adjusted_model\"})\n",
    "                    .unstack(0)\n",
    "                    .droplevel(axis=1, level=0)\n",
    "                    .T.assign(\n",
    "                        pct_significant=lambda x: x[(\"SIGNIFICANT\")] / x.sum(axis=1)\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"paper count\"],\n",
    "        ),\n",
    "    ],\n",
    "    axis=1,\n",
    ").style.format(\"{:,.2%}\", subset = [\n",
    "    ('count raw','pct'),\n",
    "    ('count paper raw','pct'),\n",
    "    ('paper count','pct_significant')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### kyoto, financial_crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            (\n",
    "                                df.groupby(\"kyoto\")\n",
    "                                .agg({\"target\": \"value_counts\"})\n",
    "                                .unstack(0)\n",
    "                            ).rename(columns={\"target\": \"kyoto\"}),\n",
    "                            (\n",
    "                                df.groupby(\"financial_crisis\")\n",
    "                                .agg({\"target\": \"value_counts\"})\n",
    "                                .unstack(0)\n",
    "                            ).rename(columns={\"target\": \"financial_crisis\"}),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    .T.reset_index()\n",
    "                    .rename(columns={\"kyoto\": \"is_dummy\", \"level_0\": \"origin\"})\n",
    "                    .set_index([\"origin\", \"is_dummy\"])\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"SIGNIFICANT\")] / x.sum(axis=1),\n",
    "                        total = lambda x: x['NOT_SIGNIFICANT'] + x['SIGNIFICANT'],\n",
    "                        pct_total = lambda x: x['total']/x.groupby(['origin'])['total'].transform('sum')\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            (\n",
    "                                df.groupby([\"kyoto\", \"target\"])\n",
    "                                .agg({\"id_source\": \"nunique\"})\n",
    "                                .rename(columns={\"id_source\": \"kyoto\"})\n",
    "                                .unstack(0)\n",
    "                            ),\n",
    "                            (\n",
    "                                df.groupby([\"financial_crisis\", \"target\"])\n",
    "                                .agg({\"id_source\": \"nunique\"})\n",
    "                                .rename(columns={\"id_source\": \"financial_crisis\"})\n",
    "                                .unstack(0)\n",
    "                            )\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    .T.reset_index()\n",
    "                    .rename(columns={\"kyoto\": \"is_dummy\", \"level_0\": \"origin\"})\n",
    "                    .set_index([\"origin\", \"is_dummy\"])\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"SIGNIFICANT\")] / x.sum(axis=1),\n",
    "                        total = lambda x: x['NOT_SIGNIFICANT'] + x['SIGNIFICANT'],\n",
    "                        pct_total = lambda x: x['total']/x.groupby(['origin'])['total'].transform('sum')\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"paper count\"],\n",
    "        ),\n",
    "    ],\n",
    "    axis=1,\n",
    ").style.format(\"{:,.2%}\", subset = [\n",
    "    ('count','pct_significant'),\n",
    "    ('count','pct_total'),\n",
    "    ('paper count','pct_significant'),\n",
    "    ('paper count','pct_total')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### publication_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"publication_year\")\n",
    "                    .agg({\"publication_year\": \"count\"})\n",
    "                    .rename(columns={\"publication_year\": \"count\"})\n",
    "                    .assign(pct=lambda x: x[\"count\"] / np.sum(x[\"count\"]))\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count raw\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"publication_year\")\n",
    "                    .agg({\"target\": \"value_counts\"})\n",
    "                    .unstack(0)\n",
    "                    .rename(columns={\"target\": \"publication_year\"})\n",
    "                    .droplevel(axis=1, level=0)\n",
    "                    .T\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count\"],\n",
    "        ),\n",
    "         pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"publication_year\")\n",
    "                    .agg({\"id_source\": \"nunique\"})\n",
    "                    .rename(columns={\"id_source\": \"count\"})\n",
    "                    .assign(pct=lambda x: x[\"count\"] / np.sum(x[\"count\"]))\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count paper raw\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby([\"publication_year\", \"target\"])\n",
    "                    .agg({\"id_source\": \"nunique\"})\n",
    "                    .rename(columns={\"id_source\": \"publication_year\"})\n",
    "                    .unstack(0)\n",
    "                    .droplevel(axis=1, level=0)\n",
    "                    .T\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"SIGNIFICANT\")] / x.sum(axis=1)\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"paper count\"],\n",
    "        ),\n",
    "    ],\n",
    "    axis=1,\n",
    ").style.format(\"{:,.2%}\", subset = [\n",
    "    ('count raw','pct'),\n",
    "    ('count paper raw','pct'),\n",
    "    ('paper count','pct_significant')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### is_open_access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            (\n",
    "                                df.groupby(\"is_open_access\")\n",
    "                                .agg({\"target\": \"value_counts\"})\n",
    "                                .unstack(0)\n",
    "                            ).rename(columns={\"target\": \"is_open_access\"}),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    .T.reset_index()\n",
    "                    .rename(columns={\"is_open_access\": \"is_dummy\", \"level_0\": \"origin\"})\n",
    "                    .set_index([\"origin\", \"is_dummy\"])\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"SIGNIFICANT\")] / x.sum(axis=1),\n",
    "                        total = lambda x: x['NOT_SIGNIFICANT'] + x['SIGNIFICANT'],\n",
    "                        pct_total = lambda x: x['total']/x.groupby(['origin'])['total'].transform('sum')\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            (\n",
    "                                df.groupby([\"is_open_access\", \"target\"])\n",
    "                                .agg({\"id_source\": \"nunique\"})\n",
    "                                .rename(columns={\"id_source\": \"is_open_access\"})\n",
    "                                .unstack(0)\n",
    "                            )\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    .T.reset_index()\n",
    "                    .rename(columns={\"is_open_access\": \"is_dummy\", \"level_0\": \"origin\"})\n",
    "                    .set_index([\"origin\", \"is_dummy\"])\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"SIGNIFICANT\")] / x.sum(axis=1),\n",
    "                        total = lambda x: x['NOT_SIGNIFICANT'] + x['SIGNIFICANT'],\n",
    "                        pct_total = lambda x: x['total']/x.groupby(['origin'])['total'].transform('sum')\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"paper count\"],\n",
    "        ),\n",
    "    ],\n",
    "    axis=1,\n",
    ").style.format(\"{:,.2%}\", subset = [\n",
    "    ('count','pct_significant'),\n",
    "    ('count','pct_total'),\n",
    "    ('paper count','pct_significant'),\n",
    "    ('paper count','pct_total')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### region_journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"region_journal\")\n",
    "                    .agg({\"region_journal\": \"count\"})\n",
    "                    .rename(columns={\"region_journal\": \"count\"})\n",
    "                    .assign(pct=lambda x: x[\"count\"] / np.sum(x[\"count\"]))\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count raw\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"region_journal\")\n",
    "                    .agg({\"target\": \"value_counts\"})\n",
    "                    .unstack(0)\n",
    "                    .rename(columns={\"target\": \"region_journal\"})\n",
    "                    .droplevel(axis=1, level=0)\n",
    "                    .T\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count\"],\n",
    "        ),\n",
    "         pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"region_journal\")\n",
    "                    .agg({\"id_source\": \"nunique\"})\n",
    "                    .rename(columns={\"id_source\": \"count\"})\n",
    "                    .assign(pct=lambda x: x[\"count\"] / np.sum(x[\"count\"]))\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count paper raw\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby([\"region_journal\", \"target\"])\n",
    "                    .agg({\"id_source\": \"nunique\"})\n",
    "                    .rename(columns={\"id_source\": \"region_journal\"})\n",
    "                    .unstack(0)\n",
    "                    .droplevel(axis=1, level=0)\n",
    "                    .T\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"SIGNIFICANT\")] / x.sum(axis=1)\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"paper count\"],\n",
    "        ),\n",
    "    ],\n",
    "    axis=1,\n",
    ").style.format(\"{:,.2%}\", subset = [\n",
    "    ('count raw','pct'),\n",
    "    ('count paper raw','pct'),\n",
    "    ('paper count','pct_significant')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"providers\")\n",
    "                    .agg({\"providers\": \"count\"})\n",
    "                    .rename(columns={\"providers\": \"count\"})\n",
    "                    .assign(pct=lambda x: x[\"count\"] / np.sum(x[\"count\"]))\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count raw\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"providers\")\n",
    "                    .agg({\"target\": \"value_counts\"})\n",
    "                    .unstack(0)\n",
    "                    .rename(columns={\"target\": \"providers\"})\n",
    "                    .droplevel(axis=1, level=0)\n",
    "                    .T\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count\"],\n",
    "        ),\n",
    "         pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"providers\")\n",
    "                    .agg({\"id_source\": \"nunique\"})\n",
    "                    .rename(columns={\"id_source\": \"count\"})\n",
    "                    .assign(pct=lambda x: x[\"count\"] / np.sum(x[\"count\"]))\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count paper raw\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby([\"providers\", \"target\"])\n",
    "                    .agg({\"id_source\": \"nunique\"})\n",
    "                    .rename(columns={\"id_source\": \"providers\"})\n",
    "                    .unstack(0)\n",
    "                    .droplevel(axis=1, level=0)\n",
    "                    .T\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"SIGNIFICANT\")] / x.sum(axis=1)\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"paper count\"],\n",
    "        ),\n",
    "    ],\n",
    "    axis=1,\n",
    ").style.format(\"{:,.2%}\", subset = [\n",
    "    ('count raw','pct'),\n",
    "    ('count paper raw','pct'),\n",
    "    ('paper count','pct_significant')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"regions\")\n",
    "                    .agg({\"regions\": \"count\"})\n",
    "                    .rename(columns={\"regions\": \"count\"})\n",
    "                    .assign(pct=lambda x: x[\"count\"] / np.sum(x[\"count\"]))\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count raw\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"regions\")\n",
    "                    .agg({\"target\": \"value_counts\"})\n",
    "                    .unstack(0)\n",
    "                    .rename(columns={\"target\": \"regions\"})\n",
    "                    .droplevel(axis=1, level=0)\n",
    "                    .T\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count\"],\n",
    "        ),\n",
    "         pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"regions\")\n",
    "                    .agg({\"id_source\": \"nunique\"})\n",
    "                    .rename(columns={\"id_source\": \"count\"})\n",
    "                    .assign(pct=lambda x: x[\"count\"] / np.sum(x[\"count\"]))\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count paper raw\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby([\"regions\", \"target\"])\n",
    "                    .agg({\"id_source\": \"nunique\"})\n",
    "                    .rename(columns={\"id_source\": \"regions\"})\n",
    "                    .unstack(0)\n",
    "                    .droplevel(axis=1, level=0)\n",
    "                    .T\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"SIGNIFICANT\")] / x.sum(axis=1)\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"paper count\"],\n",
    "        ),\n",
    "    ],\n",
    "    axis=1,\n",
    ").style.format(\"{:,.2%}\", subset = [\n",
    "    ('count raw','pct'),\n",
    "    ('count paper raw','pct'),\n",
    "    ('paper count','pct_significant')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### sjr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "(df.groupby(\"target\").agg({\"sjr\": \"describe\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### CNRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"rank_digit\")\n",
    "                    .agg({\"rank_digit\": \"count\"})\n",
    "                    .rename(columns={\"rank_digit\": \"count\"})\n",
    "                    .assign(pct=lambda x: x[\"count\"] / np.sum(x[\"count\"]))\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count raw\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"rank_digit\")\n",
    "                    .agg({\"target\": \"value_counts\"})\n",
    "                    .unstack(0)\n",
    "                    .rename(columns={\"target\": \"rank_digit\"})\n",
    "                    .droplevel(axis=1, level=0)\n",
    "                    .T\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count\"],\n",
    "        ),\n",
    "         pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"rank_digit\")\n",
    "                    .agg({\"id_source\": \"nunique\"})\n",
    "                    .rename(columns={\"id_source\": \"count\"})\n",
    "                    .assign(pct=lambda x: x[\"count\"] / np.sum(x[\"count\"]))\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count paper raw\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby([\"rank_digit\", \"target\"])\n",
    "                    .agg({\"id_source\": \"nunique\"})\n",
    "                    .rename(columns={\"id_source\": \"rank_digit\"})\n",
    "                    .unstack(0)\n",
    "                    .droplevel(axis=1, level=0)\n",
    "                    .T\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"SIGNIFICANT\")] / x.sum(axis=1)\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"paper count\"],\n",
    "        ),\n",
    "    ],\n",
    "    axis=1,\n",
    ").style.format(\"{:,.2%}\", subset = [\n",
    "    ('count raw','pct'),\n",
    "    ('count paper raw','pct'),\n",
    "    ('paper count','pct_significant')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sorted(list(df.loc[lambda x: x['rank_digit'].isin(['5'])]['publication_name'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"d_rank_digit\")\n",
    "                    .agg({\"d_rank_digit\": \"count\"})\n",
    "                    .rename(columns={\"d_rank_digit\": \"count\"})\n",
    "                    .assign(pct=lambda x: x[\"count\"] / np.sum(x[\"count\"]))\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count raw\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"d_rank_digit\")\n",
    "                    .agg({\"target\": \"value_counts\"})\n",
    "                    .unstack(0)\n",
    "                    .rename(columns={\"target\": \"d_rank_digit\"})\n",
    "                    .droplevel(axis=1, level=0)\n",
    "                    .T\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count\"],\n",
    "        ),\n",
    "         pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"d_rank_digit\")\n",
    "                    .agg({\"id_source\": \"nunique\"})\n",
    "                    .rename(columns={\"id_source\": \"count\"})\n",
    "                    .assign(pct=lambda x: x[\"count\"] / np.sum(x[\"count\"]))\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count paper raw\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby([\"d_rank_digit\", \"target\"])\n",
    "                    .agg({\"id_source\": \"nunique\"})\n",
    "                    .rename(columns={\"id_source\": \"d_rank_digit\"})\n",
    "                    .unstack(0)\n",
    "                    .droplevel(axis=1, level=0)\n",
    "                    .T\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"SIGNIFICANT\")] / x.sum(axis=1)\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"paper count\"],\n",
    "        ),\n",
    "    ],\n",
    "    axis=1,\n",
    ").style.format(\"{:,.2%}\", subset = [\n",
    "    ('count raw','pct'),\n",
    "    ('count paper raw','pct'),\n",
    "    ('paper count','pct_significant')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### complexity model\n",
    "\n",
    "- lag\n",
    "- interaction_term\n",
    "- quadratic_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            (\n",
    "                                df.groupby(\"lag\")\n",
    "                                .agg({\"target\": \"value_counts\"})\n",
    "                                .unstack(0)\n",
    "                            ).rename(columns={\"target\": \"lag\"}),\n",
    "                            (\n",
    "                                df.groupby(\"interaction_term\")\n",
    "                                .agg({\"target\": \"value_counts\"})\n",
    "                                .unstack(0)\n",
    "                            ).rename(columns={\"target\": \"interaction_term\"}),\n",
    "                            (\n",
    "                                df.groupby(\"quadratic_term\")\n",
    "                                .agg({\"target\": \"value_counts\"})\n",
    "                                .unstack(0)\n",
    "                            ).rename(columns={\"target\": \"quadratic_term\"}),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    .T.reset_index()\n",
    "                    .rename(columns={\"lag\": \"is_dummy\", \"level_0\": \"origin\"})\n",
    "                    .set_index([\"origin\", \"is_dummy\"])\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"SIGNIFICANT\")] / x.sum(axis=1),\n",
    "                         total = lambda x: x['NOT_SIGNIFICANT'] + x['SIGNIFICANT'],\n",
    "                        pct_total = lambda x: x['total']/x.groupby(['origin'])['total'].transform('sum')\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                            (\n",
    "                                df.groupby([\"lag\", \"target\"])\n",
    "                                .agg({\"id_source\": \"nunique\"})\n",
    "                                .rename(columns={\"id_source\": \"lag\"})\n",
    "                                .unstack(0)\n",
    "                            ),\n",
    "                            (\n",
    "                                df.groupby([\"interaction_term\", \"target\"])\n",
    "                                .agg({\"id_source\": \"nunique\"})\n",
    "                                .rename(columns={\"id_source\": \"interaction_term\"})\n",
    "                                .unstack(0)\n",
    "                            ),\n",
    "                            (\n",
    "                                df.groupby([\"quadratic_term\", \"target\"])\n",
    "                                .agg({\"id_source\": \"nunique\"})\n",
    "                                .rename(columns={\"id_source\": \"quadratic_term\"})\n",
    "                                .unstack(0)\n",
    "                            ),\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    .T.reset_index()\n",
    "                    .rename(columns={\"lag\": \"is_dummy\", \"level_0\": \"origin\"})\n",
    "                    .set_index([\"origin\", \"is_dummy\"])\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"SIGNIFICANT\")] / x.sum(axis=1),\n",
    "                        total = lambda x: x['NOT_SIGNIFICANT'] + x['SIGNIFICANT'],\n",
    "                        pct_total = lambda x: x['total']/x.groupby(['origin'])['total'].transform('sum')\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"paper count\"],\n",
    "        ),\n",
    "    ],\n",
    "    axis=1,\n",
    ").style.format(\"{:,.2%}\", subset = [\n",
    "    ('count','pct_significant'),\n",
    "    ('count','pct_total'),\n",
    "    ('paper count','pct_significant'),\n",
    "    ('paper count','pct_total'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "df['interaction_term'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### nb_authors, pct_female, pct_esg_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"nb_authors\")\n",
    "                    .agg({\"nb_authors\": \"count\"})\n",
    "                    .rename(columns={\"nb_authors\": \"count\"})\n",
    "                    .assign(pct=lambda x: x[\"count\"] / np.sum(x[\"count\"]))\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count raw\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"nb_authors\")\n",
    "                    .agg({\"target\": \"value_counts\"})\n",
    "                    .unstack(0)\n",
    "                    .rename(columns={\"target\": \"nb_authors\"})\n",
    "                    .droplevel(axis=1, level=0)\n",
    "                    .T\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count\"],\n",
    "        ),\n",
    "         pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"nb_authors\")\n",
    "                    .agg({\"id_source\": \"nunique\"})\n",
    "                    .rename(columns={\"id_source\": \"count\"})\n",
    "                    .assign(pct=lambda x: x[\"count\"] / np.sum(x[\"count\"]))\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count paper raw\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby([\"nb_authors\", \"target\"])\n",
    "                    .agg({\"id_source\": \"nunique\"})\n",
    "                    .rename(columns={\"id_source\": \"nb_authors\"})\n",
    "                    .unstack(0)\n",
    "                    .droplevel(axis=1, level=0)\n",
    "                    .T\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"SIGNIFICANT\")] / x.sum(axis=1)\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"paper count\"],\n",
    "        ),\n",
    "    ],\n",
    "    axis=1,\n",
    ").style.format(\"{:,.2%}\", subset = [\n",
    "    ('count raw','pct'),\n",
    "    ('count paper raw','pct'),\n",
    "    ('paper count','pct_significant')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "(df.groupby(\"target\").agg({\"pct_female\": \"describe\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "(df.groupby(\"target\").agg({\"pct_esg\": \"describe\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### sentiment, cluster_w_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"sentiment\")\n",
    "                    .agg({\"sentiment\": \"count\"})\n",
    "                    .rename(columns={\"sentiment\": \"count\"})\n",
    "                    .assign(pct=lambda x: x[\"count\"] / np.sum(x[\"count\"]))\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count raw\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"sentiment\")\n",
    "                    .agg({\"target\": \"value_counts\"})\n",
    "                    .unstack(0)\n",
    "                    .rename(columns={\"target\": \"sentiment\"})\n",
    "                    .droplevel(axis=1, level=0)\n",
    "                    .T\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count\"],\n",
    "        ),\n",
    "         pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"sentiment\")\n",
    "                    .agg({\"id_source\": \"nunique\"})\n",
    "                    .rename(columns={\"id_source\": \"count\"})\n",
    "                    .assign(pct=lambda x: x[\"count\"] / np.sum(x[\"count\"]))\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count paper raw\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby([\"sentiment\", \"target\"])\n",
    "                    .agg({\"id_source\": \"nunique\"})\n",
    "                    .rename(columns={\"id_source\": \"sentiment\"})\n",
    "                    .unstack(0)\n",
    "                    .droplevel(axis=1, level=0)\n",
    "                    .T\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"SIGNIFICANT\")] / x.sum(axis=1)\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"paper count\"],\n",
    "        ),\n",
    "    ],\n",
    "    axis=1,\n",
    ").style.format(\"{:,.2%}\", subset = [\n",
    "    ('count raw','pct'),\n",
    "    ('count paper raw','pct'),\n",
    "    ('paper count','pct_significant')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"cluster_w_emb\")\n",
    "                    .agg({\"cluster_w_emb\": \"count\"})\n",
    "                    .rename(columns={\"cluster_w_emb\": \"count\"})\n",
    "                    .assign(pct=lambda x: x[\"count\"] / np.sum(x[\"count\"]))\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count raw\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"cluster_w_emb\")\n",
    "                    .agg({\"target\": \"value_counts\"})\n",
    "                    .unstack(0)\n",
    "                    .rename(columns={\"target\": \"cluster_w_emb\"})\n",
    "                    .droplevel(axis=1, level=0)\n",
    "                    .T\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count\"],\n",
    "        ),\n",
    "         pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby(\"cluster_w_emb\")\n",
    "                    .agg({\"id_source\": \"nunique\"})\n",
    "                    .rename(columns={\"id_source\": \"count\"})\n",
    "                    .assign(pct=lambda x: x[\"count\"] / np.sum(x[\"count\"]))\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"count paper raw\"],\n",
    "        ),\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    df.groupby([\"cluster_w_emb\", \"target\"])\n",
    "                    .agg({\"id_source\": \"nunique\"})\n",
    "                    .rename(columns={\"id_source\": \"cluster_w_emb\"})\n",
    "                    .unstack(0)\n",
    "                    .droplevel(axis=1, level=0)\n",
    "                    .T\n",
    "                    .assign(\n",
    "                        pct_significant=lambda x: x[(\"SIGNIFICANT\")] / x.sum(axis=1)\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            axis=1,\n",
    "            keys=[\"paper count\"],\n",
    "        ),\n",
    "    ],\n",
    "    axis=1,\n",
    ").style.format(\"{:,.2%}\", subset = [\n",
    "    ('count raw','pct'),\n",
    "    ('count paper raw','pct'),\n",
    "    ('paper count','pct_significant')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "(\n",
    "            df.groupby([\"cluster_w_emb\", \"environmental\"])\n",
    "            .agg({\"target\": \"value_counts\"})\n",
    "            .unstack(0)\n",
    "     .rename(columns={\"target\": \"environmental\"})\n",
    "    .T\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        (\n",
    "            df.groupby([\"cluster_w_emb\", \"environmental\"])\n",
    "            .agg({\"target\": \"value_counts\"})\n",
    "            .unstack(0)\n",
    "            .rename(columns={\"target\": \"environmental\"})\n",
    "            .T\n",
    "        ),\n",
    "        (\n",
    "            df.groupby([\"cluster_w_emb\", \"social\"])\n",
    "            .agg({\"target\": \"value_counts\"})\n",
    "            .unstack(0)\n",
    "            .rename(columns={\"target\": \"social\"})\n",
    "            .T\n",
    "        ),\n",
    "        (\n",
    "            df.groupby([\"cluster_w_emb\", \"governance\"])\n",
    "            .agg({\"target\": \"value_counts\"})\n",
    "            .unstack(0)\n",
    "            .rename(columns={\"target\": \"governance\"})\n",
    "            .T\n",
    "        ),\n",
    "    ],\n",
    "    axis=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Correlation among covariates\n",
    "\n",
    "- publication_year and mid_year are highly correlated, so cannot use them simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = (\n",
    "    df\n",
    "    .reindex(columns = [\n",
    "        'publication_year',\n",
    "        'mid_year',\n",
    "        \"sjr\",\n",
    "        \"windows\",\n",
    "        \"nb_authors\",\n",
    "        \"pct_female\",\n",
    "        \"pct_esg\"\n",
    "    ])\n",
    "    .corr()\n",
    ")\n",
    "(\n",
    "    corr\n",
    "    .where(np.triu(np.ones_like(corr, dtype=bool)))\n",
    "    .T\n",
    "    .style\n",
    "    .format(\"{0:,.2f}\",na_rep=\"-\")\n",
    "    #.background_gradient()\n",
    "    #.applymap(lambda x: 'color: transparent' if pd.isnull(x) else '')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .reindex(columns = ['publication_year', 'mid_year'])\n",
    "    .plot\n",
    "    .scatter(x = 'mid_year', y = 'publication_year', figsize= (6,6) )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Schema Latex table\n",
    "\n",
    "To rename a variable, please use the following template:\n",
    "\n",
    "```\n",
    "{\n",
    "    'old':'XX',\n",
    "    'new':'XX_1'\n",
    "    }\n",
    "```\n",
    "\n",
    "if you need to pass a latex format with `\\`, you need to duplicate it for instance, `\\text` becomes `\\\\text:\n",
    "\n",
    "```\n",
    "{\n",
    "    'old':'working\\_capital\\_i',\n",
    "    'new':'\\\\text{working capital}_i'\n",
    "    }\n",
    "```\n",
    "\n",
    "Then add it to the key `to_rename`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "add_to_dic = False\n",
    "if add_to_dic:\n",
    "    if os.path.exists(\"schema_table.json\"):\n",
    "        os.remove(\"schema_table.json\")\n",
    "    data = {'to_rename':[], 'to_remove':[]}\n",
    "    dic_rename = [\n",
    "        {\n",
    "        'old':'working\\_capital\\_i',\n",
    "        'new':'\\\\text{working capital}_i'\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    data['to_rename'].extend(dic_rename)\n",
    "    with open('schema_table.json', 'w') as outfile:\n",
    "        json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(parent_path, 'utils'))\n",
    "import latex.latex_beautify as lb\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge r-lmtest -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "R"
   },
   "outputs": [],
   "source": [
    "options(warn=-1)\n",
    "library(tidyverse)\n",
    "library(\"sandwich\")\n",
    "library(\"lmtest\")\n",
    "#library(lfe)\n",
    "#library(lazyeval)\n",
    "#library(nnet)\n",
    "library('progress')\n",
    "path = \"../../../utils/latex/table_golatex.R\"\n",
    "source(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "R"
   },
   "outputs": [],
   "source": [
    "%get df_path\n",
    "\n",
    "normalit<-function(m){\n",
    "   (m - min(m))/(max(m)-min(m))\n",
    " }\n",
    "df_final <- read_csv(df_path) %>%\n",
    "mutate_if(is.character, as.factor) %>%\n",
    "\n",
    "mutate(\n",
    "    adjusted_model = relevel(adjusted_model, ref='POOLED OLS'),\n",
    "    #adjusted_dependent = relevel(adjusted_dependent, ref='OTHER'),\n",
    "    id_source = as.factor(id_source),\n",
    "    governance = relevel(as.factor(governance), ref = 'NO'),\n",
    "    social = relevel(as.factor(social), ref = 'NO'),\n",
    "    environmental =relevel(as.factor(environmental), ref = 'NO'),\n",
    "    financial_crisis =relevel(as.factor(financial_crisis), ref = 'NO'),\n",
    "    kyoto =relevel(as.factor(kyoto), ref = 'NO'),\n",
    "    target =relevel(as.factor(target), ref = 'NOT_SIGNIFICANT'),\n",
    "    regions =relevel(as.factor(regions), ref = 'WORLDWIDE'),\n",
    "    is_open_access =relevel(as.factor(is_open_access), ref = 'NO'),\n",
    "    sentiment =relevel(as.factor(sentiment), ref = 'NEGATIVE'),\n",
    "    region_journal =relevel(as.factor(region_journal), ref = 'NORTHERN AMERICA'),\n",
    "    pct_esg_1 = normalit(pct_esg),\n",
    "    esg_1 =  normalit(esg),\n",
    "    sjr_1 =  normalit(sjr),\n",
    "    cluster_w_emb = relevel(as.factor(cluster_w_emb), ref = 'CLUSTER_1'),\n",
    "    citation_count_1 = normalit(citation_count),\n",
    "    providers = relevel(as.factor(providers), ref = 'NOT_MSCI'),\n",
    "    rank_digit = relevel(as.factor(rank_digit), ref = '1'),\n",
    "    d_rank_digit = relevel(as.factor(d_rank_digit), ref = 'rank_2345')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "R"
   },
   "outputs": [],
   "source": [
    "glimpse(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "R"
   },
   "outputs": [],
   "source": [
    "transpose(df_final %>% \n",
    "    select_if(function(x) any(is.na(x))) %>% \n",
    "    summarise_each(funs(sum(is.na(.)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "R"
   },
   "source": [
    "GLM does not clustered the standard error so, we compute it by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "R"
   },
   "outputs": [],
   "source": [
    "se_robust <- function(x)\n",
    "  coeftest(x, vcov. = sandwich::sandwich\n",
    "          )[, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "R"
   },
   "source": [
    "# Table 1: Probit\n",
    "\n",
    "$$\n",
    "\\mathrm{P}\\left(\\text { Significant }_{\\mathrm{ib}}=\\mathrm{significant}\\right)=\\mathrm{\\beta}_{0} + \n",
    "\\mathrm{\\beta}_{1}\\text { ESG }_{\\mathrm{ib}}+ \n",
    "\\mathrm{\\beta}_{2}\\text { Kyoto }_{\\mathrm{i}} +\n",
    "\\mathrm{\\beta}_{3}\\text { Financial crisis }_{\\mathrm{i}} +\n",
    "\\mathrm{\\beta}_{4}\\text { Publication year }_{\\mathrm{i}} + \n",
    "\\mathrm{\\beta}_{5}\\text { windows }_{\\mathrm{i}} +\n",
    "\\mathrm{\\beta}_{6}\\text { mid-year }_{\\mathrm{i}} +\n",
    "\\mathrm{\\beta}_{7}\\text { region }_{\\mathrm{ib}}\n",
    "+\\epsilon _{\\mathrm{ib}}\n",
    "$$\n",
    "\n",
    "- robust standard error\n",
    "- Cannot compute clustered standard error if we add features without variation among the c luster (i.e `n`, or journal information)\n",
    "\n",
    "## Variable construction\n",
    "\n",
    "\n",
    "* Significant: If in the table, p-value below .1, then significant else not significant\n",
    "* The variable adjusted_independent is too imbalanced, and we are interested in only:\n",
    "  * SOCIAL\n",
    "  * ENVIRONMENTAL\n",
    "  * GOVERNANCE\n",
    "- d_rang_digit: CNRS has 4 categories, ranging from 1 to 4. We added a 5th category for the missing one. The dummy compare the top journals (rank 1) vs. the others\n",
    "* So need to create three underlying dummy variables: rules below\n",
    "  * Source low-level variable: https://docs.google.com/spreadsheets/d/1d66_CVtWni7wmKlIMcpaoanvT2ghmjbXARiHgnLWvUw/edit#gid=146632716&range=B126\n",
    "  * SOCIAL if adjusted_independent : \n",
    "    * ENVIRONMENTAL AND SOCIAL\n",
    "    * SOCIAL\n",
    "    * CSP\n",
    "    * CSR\n",
    "    * ENVIRONMENTAL, SOCIAL and GOVERNANCE\n",
    "  * ENVIRONMENTAL if adjusted_independent :\n",
    "    * ENVIRONMENTAL\n",
    "    * ENVIRONMENTAL AND SOCIAL\n",
    "    * ENVIRONMENTAL, SOCIAL and GOVERNANCE\n",
    "  * GOVERNANCE if adjusted_independent :\n",
    "    * GOVERNANCE\n",
    "    * ENVIRONMENTAL, SOCIAL and GOVERNANCE\n",
    "- adjusted_model: https://docs.google.com/spreadsheets/d/1d66_CVtWni7wmKlIMcpaoanvT2ghmjbXARiHgnLWvUw/edit#gid=793443705&range=B34\n",
    "- adjusted_dependent: https://docs.google.com/spreadsheets/d/1d66_CVtWni7wmKlIMcpaoanvT2ghmjbXARiHgnLWvUw/edit#gid=450174628&range=B59\n",
    "- Region:\n",
    "    - AFRICA: 'Cameroon', 'Egypt', 'Libya', 'Morocco', 'Nigeria'\n",
    "    - ASIA AND PACIFIC:  'India', 'Indonesia', 'Taiwan', 'Vietnam', \n",
    "        'Australia', 'China', 'Iran', 'Malaysia', \n",
    "        'Pakistan', 'South Korea', 'Bangladesh'\n",
    "    - EUROPE: 'Spain', '20 European countries', \n",
    "        'United Kingdom', 'France', 'Germany, Italy, the Netherlands and United Kingdom', \n",
    "        'Turkey', 'UK'\n",
    "    - LATIN AMERICA: 'Latin America', 'Brazil'\n",
    "    - NORTH AMERICA: 'USA', 'US', 'U.S.', 'Canada'\n",
    "    - ELSE WORLDWIDE\n",
    "- Kyoto first_date_of_observations >= 1997 THEN TRUE ELSE FALSE ,\n",
    "- Financial crisis first_date_of_observations >= 2009 THEN TRUE ELSE FALSE \n",
    "- windows: last_date_of_observations - first_date_of_observations\n",
    "- mid-year: last_date_of_observations - (windows/2)\n",
    "- is_open_access: True if the journal is an open access publication\n",
    "- region_journal: Region journal\n",
    "    - Europe\n",
    "        - Eastern Europe\n",
    "        - Western Europe\n",
    "    - Northern America\n",
    "- providers: If `csr_20_categories` equals to 'MSCI'  then \"MCSI\" else \"NOT_MSCI\". MSCI is the main ESG's data provider\n",
    "- nb_authors: Number of authors\n",
    "- pct_female: Percentage of female authors\n",
    "- pct_esg_1: ESG expert score calculated as the number of publications labeled as ESG over the total number of publications for all the authors of the paper\n",
    "- Sentiment: Overall feeling of the abstract. Positive means the abstract tend to have more words associated with a positive connotation\n",
    "- cluster_w_emb: 3 clusters computed using the words in the abstract (embeddings), the number of verbs, noun,s and adjectives but also the size of the abstract. \n",
    "\n",
    "\n",
    "## note about Probit \n",
    "\n",
    "TO estimate a probit, use `probit` link function.  For logistic regression, use `binomial`\n",
    "\n",
    "- Reason Probit instead of Logit\n",
    "    - [What is the Difference Between Logit and Probit Models?](https://tutorials.methodsconsultants.com/posts/what-is-the-difference-between-logit-and-probit-models/)\n",
    "\n",
    "Logit and probit differ in how they define $f(∗)$. The logit model uses something called the cumulative distribution function of the logistic distribution. The probit model uses something called the cumulative distribution function of the standard normal distribution to define $f(∗)$.\n",
    "\n",
    "Probit models can be generalized to account for non-constant error variances in more advanced econometric settings (known as heteroskedastic probit models)\n",
    "\n",
    "## How to read\n",
    "\n",
    "**Comparison group**\n",
    "\n",
    "- Always `OTHER`\n",
    "- Target: `SIGNIFICANT`\n",
    "- regions: `WORLDWIDE`\n",
    "- cnrs_ranking: `0`\n",
    "\n",
    "**Odd ratio**\n",
    "\n",
    "- Categorical:\n",
    "    - Keeping all other variables constant, if the analysis uses FIXED EFFECT model, there are 2.71 times more likely to stay in the NEGATIVE sign category as compared to the OTHER model category. The coefficient, however, is not significant. (Col 1)\n",
    "- Continuous:\n",
    "    - Keeping all other variables constant, if the SJR score increases one unit, there is 1.003 times more likely to stay in the POSITIVE sign category as compared to the OTHER model category y (the risk or odds is .2% higher). The coefficient is significant.\n",
    "    \n",
    "Here, OTHER means insignificant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "R"
   },
   "source": [
    "### Baseline table\n",
    "\n",
    "The baseline regression accounts for: \n",
    "\n",
    "```\n",
    "environmental # social governance\n",
    "           + adjusted_model  \n",
    "           + kyoto \n",
    "           + financial_crisis\n",
    "           + publication_year\n",
    "           + windows\n",
    "           + mid_year\n",
    "           + regions\n",
    "           + sjr\n",
    "           + is_open_access\n",
    "           + region_journal\n",
    "           + providers\n",
    "```\n",
    "\n",
    "**Remove categorie**\n",
    "\n",
    "Removing the categorie reduces the log-likelihood and reduce the AIC criteria (lower value, the better the model)\n",
    "\n",
    "- adjusted_model:\n",
    "    - DIFF IN DIFF\n",
    "    - INSTRUMENT\n",
    "    - LAG DEPENDENT\n",
    "    - RANDOM EFFECT\n",
    "- regions\n",
    "     - LATIN AMERICA\n",
    "     \n",
    "- Faire attention a l'économétrie -> si on controle pas correctement, on trouve un lien (OLS vs fE) -> outil -s rudimentaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "R"
   },
   "outputs": [],
   "source": [
    "### Baseline SJR\n",
    "t_0 <- glm(target ~ environmental * kyoto\n",
    "           + kyoto \n",
    "           + adjusted_model  \n",
    "           + financial_crisis\n",
    "           + windows\n",
    "           + regions\n",
    "           + providers\n",
    "           + publication_year_int\n",
    "           + is_open_access\n",
    "           + region_journal\n",
    "           + sjr\n",
    "           + sentiment\n",
    "           + nb_authors\n",
    "           + pct_female\n",
    "           + pct_esg_1,\n",
    "           data = df_final,\n",
    "           binomial(link = \"probit\")\n",
    "          )\n",
    "t_0.rrr <- exp(coef(t_0))\n",
    "\n",
    "t_1 <- glm(target ~ environmental * kyoto\n",
    "           + kyoto \n",
    "           + adjusted_model  \n",
    "           + financial_crisis\n",
    "           + windows\n",
    "           + regions\n",
    "           + providers           \n",
    "           + publication_year_int\n",
    "           + is_open_access\n",
    "           + region_journal\n",
    "           + sjr\n",
    "           + sentiment\n",
    "           + nb_authors\n",
    "           + pct_female\n",
    "           + pct_esg_1\n",
    "           + lag\n",
    "           + interaction_term\n",
    "           + quadratic_term,\n",
    "           data = df_final,\n",
    "           binomial(link = \"probit\")\n",
    "          )\n",
    "t_1.rrr <- exp(coef(t_1))\n",
    "\n",
    "### CNRS\n",
    "t_2 <- glm(target ~ environmental * kyoto\n",
    "           + kyoto \n",
    "           + adjusted_model  \n",
    "           + financial_crisis\n",
    "           + windows\n",
    "           + regions\n",
    "           + providers           \n",
    "           + publication_year_int\n",
    "           + is_open_access\n",
    "           + region_journal\n",
    "           + rank_digit\n",
    "           + sentiment\n",
    "           + nb_authors\n",
    "           + pct_female\n",
    "           + pct_esg_1,\n",
    "           data = df_final,\n",
    "           binomial(link = \"probit\")\n",
    "          )\n",
    "t_2.rrr <- exp(coef(t_2))\n",
    "\n",
    "t_3 <- glm(target ~ environmental * kyoto\n",
    "           + kyoto \n",
    "           + adjusted_model  \n",
    "           + financial_crisis\n",
    "           + windows\n",
    "           + regions\n",
    "           + providers\n",
    "           + publication_year_int\n",
    "           + is_open_access\n",
    "           + region_journal\n",
    "           + rank_digit\n",
    "           + sentiment\n",
    "           + nb_authors\n",
    "           + pct_female\n",
    "           + pct_esg_1\n",
    "           + lag\n",
    "           + interaction_term\n",
    "           + quadratic_term,\n",
    "           data = df_final,\n",
    "           binomial(link = \"probit\")\n",
    "          )\n",
    "t_3.rrr <- exp(coef(t_3))\n",
    "\n",
    "list_final = list(t_0, t_1, t_2, t_3\n",
    "                 )\n",
    "list_final.rrr = list(t_0.rrr,t_1.rrr ,t_2.rrr,t_3.rrr\n",
    "                     )\n",
    "stargazer(list_final, type = \"text\", \n",
    "  se = lapply(list_final,\n",
    "              se_robust),\n",
    "          coef=list_final.rrr,\n",
    "          style = \"qje\",\n",
    "          title = \"Effect of environemntal score of ESG on CFP\",\n",
    "        order = c(1,2,32,4, 9, 5, 6),\n",
    "          covariate.labels = c(\n",
    "    \"Environmental\",\n",
    "    \"Kyoto\",\n",
    "    \"Environmental x Kyoto\",\n",
    "    \"Fixed effect\",\n",
    "    \"Diff in Diff\",\n",
    "    \"Random effect\",\n",
    "    \"GMM\",\n",
    "    \"Instrument\",\n",
    "    \"Lag dependent\",\n",
    "    \"Other\",\n",
    "    \"Financial crisis\",\n",
    "    \"Windows\",\n",
    "    \"Regions Africa\",\n",
    "    \"Regions Europe\",\n",
    "    \"Regions Latin America\",\n",
    "    \"Regions North America\",\n",
    "    \"MSCI\",          \n",
    "    \"Publication year\",\n",
    "    \"Open access\",\n",
    "    \"Region journal\",\n",
    "    \"SJR\",\n",
    "    \"CNRS rank 2\",\n",
    "    \"CNRS rank 3\",\n",
    "    \"CNRS rank 4\",\n",
    "    \"CNRS rank 5\",\n",
    "    \"Sentiment\",\n",
    "    \"Nb authors\",\n",
    "    \"Pct female\",\n",
    "    \"ESG score\",\n",
    "    \"Lag\",\n",
    "    \"Interaction term\",\n",
    "    \"Quadratic term\"\n",
    "),\n",
    "          out=\"TABLES/table_0.txt\"\n",
    "          #out=\"sjt_corr.html\"\n",
    "         )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Generate reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "python3",
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "import os, time, shutil, urllib, ipykernel, json\n",
    "from pathlib import Path\n",
    "from notebook import notebookapp\n",
    "import sys\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent.parent.parent)\n",
    "sys.path.append(os.path.join(parent_path, 'utils'))\n",
    "import make_toc\n",
    "import create_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "python3"
   },
   "outputs": [],
   "source": [
    "name_json = 'parameters_ETL_esg_metadata.json'\n",
    "path_json = os.path.join(str(Path(path).parent.parent), 'utils',name_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "python3",
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "create_report.create_report(extension = \"html\", keep_code = True, notebookname = \"00_sign_of_effect_classification.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "python3"
   },
   "outputs": [],
   "source": [
    "### Update TOC in Github\n",
    "for p in [parent_path,\n",
    "          str(Path(path).parent),\n",
    "          #os.path.join(str(Path(path).parent), \"00_download_data_from\"),\n",
    "          #os.path.join(str(Path(path).parent.parent), \"02_data_analysis\"),\n",
    "          #os.path.join(str(Path(path).parent.parent), \"02_data_analysis\", \"00_statistical_exploration\"),\n",
    "          #os.path.join(str(Path(path).parent.parent), \"02_data_analysis\", \"01_model_estimation\"),\n",
    "         ]:\n",
    "    try:\n",
    "        os.remove(os.path.join(p, 'README.md'))\n",
    "    except:\n",
    "        pass\n",
    "    path_parameter = os.path.join(parent_path,'utils', name_json)\n",
    "    md_lines =  make_toc.create_index(cwd = p, path_parameter = path_parameter)\n",
    "    md_out_fn = os.path.join(p,'README.md')\n",
    "    \n",
    "    if p == parent_path:\n",
    "    \n",
    "        make_toc.replace_index(md_out_fn, md_lines, Header = os.path.basename(p).replace('_', ' '), add_description = True, path_parameter = path_parameter)\n",
    "    else:\n",
    "        make_toc.replace_index(md_out_fn, md_lines, Header = os.path.basename(p).replace('_', ' '), add_description = False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "nteract": {
   "version": "0.28.0"
  },
  "sos": {
   "kernels": [
    [
     "R",
     "ir",
     "R",
     "#DCDCDA",
     "r"
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ],
    [
     "python3",
     "python3",
     "python",
     "",
     {
      "name": "ipython",
      "version": 3
     }
    ]
   ],
   "version": "0.20.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
